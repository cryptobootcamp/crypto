{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "2DYKjPS5dTgP"
   },
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import OpenBlender\n",
    "import datetime\n",
    "import time\n",
    "#from sqlalchemy import create_engine\n",
    "#from config import db_password\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "z9ZQY7aCdTgc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalised_date</th>\n",
       "      <th>score</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>rolling</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normalised_date_dt64</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.893367</td>\n",
       "      <td>0.060233</td>\n",
       "      <td>0.086240</td>\n",
       "      <td>0.121112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-02</th>\n",
       "      <td>2012-01-02</td>\n",
       "      <td>2.121951</td>\n",
       "      <td>0.079707</td>\n",
       "      <td>0.818707</td>\n",
       "      <td>0.101561</td>\n",
       "      <td>0.042127</td>\n",
       "      <td>0.078388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-03</th>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>2.590909</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.817523</td>\n",
       "      <td>0.102477</td>\n",
       "      <td>0.029139</td>\n",
       "      <td>0.035952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04</th>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>2.725000</td>\n",
       "      <td>0.078175</td>\n",
       "      <td>0.833075</td>\n",
       "      <td>0.088750</td>\n",
       "      <td>0.049387</td>\n",
       "      <td>0.051089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-05</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>3.585366</td>\n",
       "      <td>0.058146</td>\n",
       "      <td>0.846829</td>\n",
       "      <td>0.094976</td>\n",
       "      <td>0.170327</td>\n",
       "      <td>0.195525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     normalised_date     score       neg       neu       pos  \\\n",
       "normalised_date_dt64                                                           \n",
       "2012-01-01                2012-01-01  3.333333  0.046400  0.893367  0.060233   \n",
       "2012-01-02                2012-01-02  2.121951  0.079707  0.818707  0.101561   \n",
       "2012-01-03                2012-01-03  2.590909  0.080000  0.817523  0.102477   \n",
       "2012-01-04                2012-01-04  2.725000  0.078175  0.833075  0.088750   \n",
       "2012-01-05                2012-01-05  3.585366  0.058146  0.846829  0.094976   \n",
       "\n",
       "                      compound   rolling  \n",
       "normalised_date_dt64                      \n",
       "2012-01-01            0.086240  0.121112  \n",
       "2012-01-02            0.042127  0.078388  \n",
       "2012-01-03            0.029139  0.035952  \n",
       "2012-01-04            0.049387  0.051089  \n",
       "2012-01-05            0.170327  0.195525  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_scores_df = pd.read_csv('Resources/reddit_comments_agg.csv')\n",
    "sentiment_scores_df = sentiment_scores_df[1:]\n",
    "sentiment_scores_df['normalised_date_dt64'] = [pd.to_datetime(x, format='%Y-%m-%d') for x in sentiment_scores_df['normalised_date']]\n",
    "sentiment_scores_df.set_index(sentiment_scores_df['normalised_date_dt64'], inplace=True)\n",
    "sentiment_scores_df2 = sentiment_scores_df.drop(columns=['normalised_date_dt64'])\n",
    "sentiment_scores_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "5o5jUE7TdTgg",
    "outputId": "019099d6-5dbd-4889-9c27-a4b54e1a6d34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of total Bitstamp data lost due to dropping rows with NaN values: 27.14%\n",
      "Ratio of Bitstamp rows dropped: 1,240,496/4,571,033\n",
      "Total dataset contains 3174.33 days worth of data. Due to data loss, we had to drop 861.46 days worth of data\n",
      "timestamp                 int64\n",
      "high                    float64\n",
      "low                     float64\n",
      "volume                  float64\n",
      "weightedPrice           float64\n",
      "Open                    float64\n",
      "Close                   float64\n",
      "timestampSTR             object\n",
      "timestampINT              int32\n",
      "timeUTC          datetime64[ns]\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>timestampSTR</th>\n",
       "      <th>timestampINT</th>\n",
       "      <th>timeUTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1325391360</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1.502</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1325391360</td>\n",
       "      <td>1325391360</td>\n",
       "      <td>2012-01-01 04:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1325431680</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.84</td>\n",
       "      <td>10.000</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.84</td>\n",
       "      <td>1325431680</td>\n",
       "      <td>1325431680</td>\n",
       "      <td>2012-01-01 15:28:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  high   low  volume  weightedPrice  Open  Close timestampSTR  \\\n",
       "0  1325391360  4.58  4.58   1.502           4.58  4.58   4.58   1325391360   \n",
       "1  1325431680  4.84  4.84  10.000           4.84  4.84   4.84   1325431680   \n",
       "\n",
       "   timestampINT             timeUTC  \n",
       "0    1325391360 2012-01-01 04:16:00  \n",
       "1    1325431680 2012-01-01 15:28:00  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in files\n",
    "df = pd.read_csv('Resources/bitcoin_data.zip')\n",
    "\n",
    "# Display initial data\n",
    "df.head(10)\n",
    "\n",
    "# keep timestamp, high, low, weighted_price\n",
    "df = df[['Timestamp', 'High', 'Low', 'Volume_(BTC)', 'Weighted_Price', 'Open', 'Close']].reset_index(drop=True)\n",
    "\n",
    "# preprocessing notes-- consider omitting data prior to Jan 1st, 2012 due to higher volatility \n",
    "# earlier in bitcoin's lifecycle\n",
    "\n",
    "# converting Timestamp column to str datatype\n",
    "df['str_timestamp'] = [str(timestamp) for timestamp in df['Timestamp']]\n",
    "df['int_timestamp'] = df['str_timestamp'].astype('int32', copy=True)\n",
    "\n",
    "# filtering df to only include rows after 01/01/2012\n",
    "df_cleaned_filtered = df.loc[df['int_timestamp'] >= 1325391360]\n",
    "df_cleaned_filtered.head(2)\n",
    "\n",
    "# context manager to suppress 1 time SettingWithCopyWarning; alternatively call .loc after timestamp conversion to avoid error\n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    df_cleaned_filtered['UTC_time'] = df_cleaned_filtered['int_timestamp'].apply(lambda x: datetime.datetime.utcfromtimestamp(x))\n",
    "\n",
    "# dropping all null values, followed by evaluation of which time periods had the greatest data loss\n",
    "final_df = df_cleaned_filtered.dropna().reset_index(drop=True)\n",
    "\n",
    "# finding percentage of data loss (null values)\n",
    "btotalLength = (len(df_cleaned_filtered))\n",
    "bpartialLength = (len(final_df))\n",
    "bValuesDropped = btotalLength - bpartialLength\n",
    "bitstampDiff = (btotalLength - bpartialLength) / btotalLength * 100 \n",
    "totalDays = round(btotalLength/1440,2)\n",
    "daysDropped = round(bValuesDropped/1440,2)\n",
    "\n",
    "print(f\"Percentage of total Bitstamp data lost due to dropping rows with NaN values: {bitstampDiff:.2f}%\")\n",
    "print(f\"Ratio of Bitstamp rows dropped: {bValuesDropped:,}/{btotalLength:,}\")\n",
    "print(f\"Total dataset contains {totalDays} days worth of data. Due to data loss, we had to drop {daysDropped} days worth of data\")\n",
    "\n",
    "final_df.head(5)\n",
    "\n",
    "# Rename columns to be sql friendly\n",
    "final_df.rename({'Timestamp':'timestamp',\n",
    "                  'High':'high',\n",
    "                  'Low':'low',\n",
    "                  'Volume_(BTC)':'volume',\n",
    "                  'Weighted_Price':'weightedPrice',\n",
    "                  'str_timestamp':'timestampSTR',\n",
    "                  'int_timestamp':'timestampINT',\n",
    "                  'UTC_time':'timeUTC'\n",
    "                 }, axis='columns', inplace=True)\n",
    "\n",
    "\n",
    "# Checking datatypes\n",
    "print(final_df.dtypes)\n",
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "YU3D3Xj8dTgi",
    "outputId": "8759a6d5-aa98-4962-ab84-fdee15676614"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>timestampSTR</th>\n",
       "      <th>timestampINT</th>\n",
       "      <th>timeUTC</th>\n",
       "      <th>normalised_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1325391360</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1.502000</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1325391360</td>\n",
       "      <td>1325391360</td>\n",
       "      <td>2012-01-01 04:16:00</td>\n",
       "      <td>2012-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1325431680</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.84</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.84</td>\n",
       "      <td>1325431680</td>\n",
       "      <td>1325431680</td>\n",
       "      <td>2012-01-01 15:28:00</td>\n",
       "      <td>2012-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1325457900</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1325457900</td>\n",
       "      <td>1325457900</td>\n",
       "      <td>2012-01-01 22:45:00</td>\n",
       "      <td>2012-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1325534640</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>19.048000</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1325534640</td>\n",
       "      <td>1325534640</td>\n",
       "      <td>2012-01-02 20:04:00</td>\n",
       "      <td>2012-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1325591100</td>\n",
       "      <td>5.32</td>\n",
       "      <td>5.32</td>\n",
       "      <td>2.419173</td>\n",
       "      <td>5.32</td>\n",
       "      <td>5.32</td>\n",
       "      <td>5.32</td>\n",
       "      <td>1325591100</td>\n",
       "      <td>1325591100</td>\n",
       "      <td>2012-01-03 11:45:00</td>\n",
       "      <td>2012-01-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  high   low     volume  weightedPrice  Open  Close timestampSTR  \\\n",
       "0  1325391360  4.58  4.58   1.502000           4.58  4.58   4.58   1325391360   \n",
       "1  1325431680  4.84  4.84  10.000000           4.84  4.84   4.84   1325431680   \n",
       "2  1325457900  5.00  5.00  10.100000           5.00  5.00   5.00   1325457900   \n",
       "3  1325534640  5.00  5.00  19.048000           5.00  5.00   5.00   1325534640   \n",
       "4  1325591100  5.32  5.32   2.419173           5.32  5.32   5.32   1325591100   \n",
       "\n",
       "   timestampINT             timeUTC normalised_date  \n",
       "0    1325391360 2012-01-01 04:16:00      2012-01-01  \n",
       "1    1325431680 2012-01-01 15:28:00      2012-01-01  \n",
       "2    1325457900 2012-01-01 22:45:00      2012-01-01  \n",
       "3    1325534640 2012-01-02 20:04:00      2012-01-02  \n",
       "4    1325591100 2012-01-03 11:45:00      2012-01-03  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['normalised_date'] = final_df['timeUTC'].dt.normalize()\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "wVgFmJZ6dTgj"
   },
   "outputs": [],
   "source": [
    "final_df['date_only'] = final_df['normalised_date']\n",
    "# aggregated_df = final_df.groupby(final_df.normalised_date.dt.date, as_index=False).mean()\n",
    "\n",
    "# use this line to keep dates as Series\n",
    "# aggregated_df1 = final_df.groupby(final_df.normalised_date.dt.date, as_index=True).mean().reset_index()\n",
    "# indices = aggregated_df1['normalised_date']\n",
    "\n",
    "# dataframe with Time Series index\n",
    "aggregated_df = final_df.groupby(final_df.normalised_date.dt.date, as_index=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>timestampINT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normalised_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01</th>\n",
       "      <td>1.325427e+09</td>\n",
       "      <td>4.806667</td>\n",
       "      <td>4.806667</td>\n",
       "      <td>7.200667</td>\n",
       "      <td>4.806667</td>\n",
       "      <td>4.806667</td>\n",
       "      <td>4.806667</td>\n",
       "      <td>1.325427e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-02</th>\n",
       "      <td>1.325535e+09</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>19.048000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.325535e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-03</th>\n",
       "      <td>1.325605e+09</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>11.004660</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>1.325605e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04</th>\n",
       "      <td>1.325682e+09</td>\n",
       "      <td>5.223333</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>11.914807</td>\n",
       "      <td>5.208159</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>5.223333</td>\n",
       "      <td>1.325682e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-05</th>\n",
       "      <td>1.325771e+09</td>\n",
       "      <td>6.286190</td>\n",
       "      <td>6.281429</td>\n",
       "      <td>4.514373</td>\n",
       "      <td>6.284127</td>\n",
       "      <td>6.281429</td>\n",
       "      <td>6.286190</td>\n",
       "      <td>1.325771e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-10</th>\n",
       "      <td>1.599739e+09</td>\n",
       "      <td>10343.558552</td>\n",
       "      <td>10334.291588</td>\n",
       "      <td>7.119214</td>\n",
       "      <td>10339.359896</td>\n",
       "      <td>10338.759708</td>\n",
       "      <td>10338.760864</td>\n",
       "      <td>1.599739e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-11</th>\n",
       "      <td>1.599826e+09</td>\n",
       "      <td>10306.071361</td>\n",
       "      <td>10299.283936</td>\n",
       "      <td>3.660672</td>\n",
       "      <td>10303.242947</td>\n",
       "      <td>10302.414327</td>\n",
       "      <td>10302.451926</td>\n",
       "      <td>1.599826e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-12</th>\n",
       "      <td>1.599912e+09</td>\n",
       "      <td>10375.469839</td>\n",
       "      <td>10370.388713</td>\n",
       "      <td>1.754713</td>\n",
       "      <td>10373.259995</td>\n",
       "      <td>10372.770434</td>\n",
       "      <td>10372.583252</td>\n",
       "      <td>1.599912e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-13</th>\n",
       "      <td>1.599999e+09</td>\n",
       "      <td>10396.386007</td>\n",
       "      <td>10390.076996</td>\n",
       "      <td>3.056612</td>\n",
       "      <td>10393.662602</td>\n",
       "      <td>10393.230099</td>\n",
       "      <td>10392.960134</td>\n",
       "      <td>1.599999e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-14</th>\n",
       "      <td>1.600042e+09</td>\n",
       "      <td>10338.820000</td>\n",
       "      <td>10332.370000</td>\n",
       "      <td>1.292006</td>\n",
       "      <td>10332.429402</td>\n",
       "      <td>10338.820000</td>\n",
       "      <td>10332.370000</td>\n",
       "      <td>1.600042e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3177 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    timestamp          high           low     volume  \\\n",
       "normalised_date                                                        \n",
       "2012-01-01       1.325427e+09      4.806667      4.806667   7.200667   \n",
       "2012-01-02       1.325535e+09      5.000000      5.000000  19.048000   \n",
       "2012-01-03       1.325605e+09      5.252500      5.252500  11.004660   \n",
       "2012-01-04       1.325682e+09      5.223333      5.200000  11.914807   \n",
       "2012-01-05       1.325771e+09      6.286190      6.281429   4.514373   \n",
       "...                       ...           ...           ...        ...   \n",
       "2020-09-10       1.599739e+09  10343.558552  10334.291588   7.119214   \n",
       "2020-09-11       1.599826e+09  10306.071361  10299.283936   3.660672   \n",
       "2020-09-12       1.599912e+09  10375.469839  10370.388713   1.754713   \n",
       "2020-09-13       1.599999e+09  10396.386007  10390.076996   3.056612   \n",
       "2020-09-14       1.600042e+09  10338.820000  10332.370000   1.292006   \n",
       "\n",
       "                 weightedPrice          Open         Close  timestampINT  \n",
       "normalised_date                                                           \n",
       "2012-01-01            4.806667      4.806667      4.806667  1.325427e+09  \n",
       "2012-01-02            5.000000      5.000000      5.000000  1.325535e+09  \n",
       "2012-01-03            5.252500      5.252500      5.252500  1.325605e+09  \n",
       "2012-01-04            5.208159      5.200000      5.223333  1.325682e+09  \n",
       "2012-01-05            6.284127      6.281429      6.286190  1.325771e+09  \n",
       "...                        ...           ...           ...           ...  \n",
       "2020-09-10        10339.359896  10338.759708  10338.760864  1.599739e+09  \n",
       "2020-09-11        10303.242947  10302.414327  10302.451926  1.599826e+09  \n",
       "2020-09-12        10373.259995  10372.770434  10372.583252  1.599912e+09  \n",
       "2020-09-13        10393.662602  10393.230099  10392.960134  1.599999e+09  \n",
       "2020-09-14        10332.429402  10338.820000  10332.370000  1.600042e+09  \n",
       "\n",
       "[3177 rows x 8 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>timestampINT</th>\n",
       "      <th>normalised_date</th>\n",
       "      <th>score</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>rolling</th>\n",
       "      <th>normalised_date_dt64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01</th>\n",
       "      <td>1.325427e+09</td>\n",
       "      <td>4.806667</td>\n",
       "      <td>4.806667</td>\n",
       "      <td>7.200667</td>\n",
       "      <td>4.806667</td>\n",
       "      <td>4.806667</td>\n",
       "      <td>4.806667</td>\n",
       "      <td>1.325427e+09</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.893367</td>\n",
       "      <td>0.060233</td>\n",
       "      <td>0.086240</td>\n",
       "      <td>0.121112</td>\n",
       "      <td>2012-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-02</th>\n",
       "      <td>1.325535e+09</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>19.048000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.325535e+09</td>\n",
       "      <td>2012-01-02</td>\n",
       "      <td>2.121951</td>\n",
       "      <td>0.079707</td>\n",
       "      <td>0.818707</td>\n",
       "      <td>0.101561</td>\n",
       "      <td>0.042127</td>\n",
       "      <td>0.078388</td>\n",
       "      <td>2012-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-03</th>\n",
       "      <td>1.325605e+09</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>11.004660</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>1.325605e+09</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>2.590909</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.817523</td>\n",
       "      <td>0.102477</td>\n",
       "      <td>0.029139</td>\n",
       "      <td>0.035952</td>\n",
       "      <td>2012-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04</th>\n",
       "      <td>1.325682e+09</td>\n",
       "      <td>5.223333</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>11.914807</td>\n",
       "      <td>5.208159</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>5.223333</td>\n",
       "      <td>1.325682e+09</td>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>2.725000</td>\n",
       "      <td>0.078175</td>\n",
       "      <td>0.833075</td>\n",
       "      <td>0.088750</td>\n",
       "      <td>0.049387</td>\n",
       "      <td>0.051089</td>\n",
       "      <td>2012-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-05</th>\n",
       "      <td>1.325771e+09</td>\n",
       "      <td>6.286190</td>\n",
       "      <td>6.281429</td>\n",
       "      <td>4.514373</td>\n",
       "      <td>6.284127</td>\n",
       "      <td>6.281429</td>\n",
       "      <td>6.286190</td>\n",
       "      <td>1.325771e+09</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>3.585366</td>\n",
       "      <td>0.058146</td>\n",
       "      <td>0.846829</td>\n",
       "      <td>0.094976</td>\n",
       "      <td>0.170327</td>\n",
       "      <td>0.195525</td>\n",
       "      <td>2012-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp      high       low     volume  weightedPrice  \\\n",
       "2012-01-01  1.325427e+09  4.806667  4.806667   7.200667       4.806667   \n",
       "2012-01-02  1.325535e+09  5.000000  5.000000  19.048000       5.000000   \n",
       "2012-01-03  1.325605e+09  5.252500  5.252500  11.004660       5.252500   \n",
       "2012-01-04  1.325682e+09  5.223333  5.200000  11.914807       5.208159   \n",
       "2012-01-05  1.325771e+09  6.286190  6.281429   4.514373       6.284127   \n",
       "\n",
       "                Open     Close  timestampINT normalised_date     score  \\\n",
       "2012-01-01  4.806667  4.806667  1.325427e+09      2012-01-01  3.333333   \n",
       "2012-01-02  5.000000  5.000000  1.325535e+09      2012-01-02  2.121951   \n",
       "2012-01-03  5.252500  5.252500  1.325605e+09      2012-01-03  2.590909   \n",
       "2012-01-04  5.200000  5.223333  1.325682e+09      2012-01-04  2.725000   \n",
       "2012-01-05  6.281429  6.286190  1.325771e+09      2012-01-05  3.585366   \n",
       "\n",
       "                 neg       neu       pos  compound   rolling  \\\n",
       "2012-01-01  0.046400  0.893367  0.060233  0.086240  0.121112   \n",
       "2012-01-02  0.079707  0.818707  0.101561  0.042127  0.078388   \n",
       "2012-01-03  0.080000  0.817523  0.102477  0.029139  0.035952   \n",
       "2012-01-04  0.078175  0.833075  0.088750  0.049387  0.051089   \n",
       "2012-01-05  0.058146  0.846829  0.094976  0.170327  0.195525   \n",
       "\n",
       "           normalised_date_dt64  \n",
       "2012-01-01           2012-01-01  \n",
       "2012-01-02           2012-01-02  \n",
       "2012-01-03           2012-01-03  \n",
       "2012-01-04           2012-01-04  \n",
       "2012-01-05           2012-01-05  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_df = pd.concat([aggregated_df,sentiment_scores_df], axis=1, join='inner')\n",
    "aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "-awacUOqdTgj",
    "outputId": "9530a0f4-51ce-476d-f834-880f0c112beb",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>timestampINT</th>\n",
       "      <th>normalised_date</th>\n",
       "      <th>score</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>rolling</th>\n",
       "      <th>normalised_date_dt64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01</th>\n",
       "      <td>1.325427e+09</td>\n",
       "      <td>4.806667</td>\n",
       "      <td>4.806667</td>\n",
       "      <td>7.200667</td>\n",
       "      <td>4.806667</td>\n",
       "      <td>4.806667</td>\n",
       "      <td>4.806667</td>\n",
       "      <td>1.325427e+09</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.893367</td>\n",
       "      <td>0.060233</td>\n",
       "      <td>0.086240</td>\n",
       "      <td>0.121112</td>\n",
       "      <td>2012-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-02</th>\n",
       "      <td>1.325535e+09</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>19.048000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.325535e+09</td>\n",
       "      <td>2012-01-02</td>\n",
       "      <td>2.121951</td>\n",
       "      <td>0.079707</td>\n",
       "      <td>0.818707</td>\n",
       "      <td>0.101561</td>\n",
       "      <td>0.042127</td>\n",
       "      <td>0.078388</td>\n",
       "      <td>2012-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-03</th>\n",
       "      <td>1.325605e+09</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>11.004660</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>5.252500</td>\n",
       "      <td>1.325605e+09</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>2.590909</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.817523</td>\n",
       "      <td>0.102477</td>\n",
       "      <td>0.029139</td>\n",
       "      <td>0.035952</td>\n",
       "      <td>2012-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04</th>\n",
       "      <td>1.325682e+09</td>\n",
       "      <td>5.223333</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>11.914807</td>\n",
       "      <td>5.208159</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>5.223333</td>\n",
       "      <td>1.325682e+09</td>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>2.725000</td>\n",
       "      <td>0.078175</td>\n",
       "      <td>0.833075</td>\n",
       "      <td>0.088750</td>\n",
       "      <td>0.049387</td>\n",
       "      <td>0.051089</td>\n",
       "      <td>2012-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-05</th>\n",
       "      <td>1.325771e+09</td>\n",
       "      <td>6.286190</td>\n",
       "      <td>6.281429</td>\n",
       "      <td>4.514373</td>\n",
       "      <td>6.284127</td>\n",
       "      <td>6.281429</td>\n",
       "      <td>6.286190</td>\n",
       "      <td>1.325771e+09</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>3.585366</td>\n",
       "      <td>0.058146</td>\n",
       "      <td>0.846829</td>\n",
       "      <td>0.094976</td>\n",
       "      <td>0.170327</td>\n",
       "      <td>0.195525</td>\n",
       "      <td>2012-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-10</th>\n",
       "      <td>1.599739e+09</td>\n",
       "      <td>10343.558552</td>\n",
       "      <td>10334.291588</td>\n",
       "      <td>7.119214</td>\n",
       "      <td>10339.359896</td>\n",
       "      <td>10338.759708</td>\n",
       "      <td>10338.760864</td>\n",
       "      <td>1.599739e+09</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.065841</td>\n",
       "      <td>0.835543</td>\n",
       "      <td>0.098622</td>\n",
       "      <td>0.170521</td>\n",
       "      <td>0.165506</td>\n",
       "      <td>2020-09-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-11</th>\n",
       "      <td>1.599826e+09</td>\n",
       "      <td>10306.071361</td>\n",
       "      <td>10299.283936</td>\n",
       "      <td>3.660672</td>\n",
       "      <td>10303.242947</td>\n",
       "      <td>10302.414327</td>\n",
       "      <td>10302.451926</td>\n",
       "      <td>1.599826e+09</td>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>1.515260</td>\n",
       "      <td>0.068309</td>\n",
       "      <td>0.821795</td>\n",
       "      <td>0.109898</td>\n",
       "      <td>0.147518</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>2020-09-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-12</th>\n",
       "      <td>1.599912e+09</td>\n",
       "      <td>10375.469839</td>\n",
       "      <td>10370.388713</td>\n",
       "      <td>1.754713</td>\n",
       "      <td>10373.259995</td>\n",
       "      <td>10372.770434</td>\n",
       "      <td>10372.583252</td>\n",
       "      <td>1.599912e+09</td>\n",
       "      <td>2020-09-12</td>\n",
       "      <td>3.245926</td>\n",
       "      <td>0.065944</td>\n",
       "      <td>0.828458</td>\n",
       "      <td>0.105636</td>\n",
       "      <td>0.152564</td>\n",
       "      <td>0.165354</td>\n",
       "      <td>2020-09-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-13</th>\n",
       "      <td>1.599999e+09</td>\n",
       "      <td>10396.386007</td>\n",
       "      <td>10390.076996</td>\n",
       "      <td>3.056612</td>\n",
       "      <td>10393.662602</td>\n",
       "      <td>10393.230099</td>\n",
       "      <td>10392.960134</td>\n",
       "      <td>1.599999e+09</td>\n",
       "      <td>2020-09-13</td>\n",
       "      <td>3.279534</td>\n",
       "      <td>0.064995</td>\n",
       "      <td>0.831902</td>\n",
       "      <td>0.103080</td>\n",
       "      <td>0.144426</td>\n",
       "      <td>0.145943</td>\n",
       "      <td>2020-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-14</th>\n",
       "      <td>1.600042e+09</td>\n",
       "      <td>10338.820000</td>\n",
       "      <td>10332.370000</td>\n",
       "      <td>1.292006</td>\n",
       "      <td>10332.429402</td>\n",
       "      <td>10338.820000</td>\n",
       "      <td>10332.370000</td>\n",
       "      <td>1.600042e+09</td>\n",
       "      <td>2020-09-14</td>\n",
       "      <td>2.699083</td>\n",
       "      <td>0.064688</td>\n",
       "      <td>0.832695</td>\n",
       "      <td>0.102602</td>\n",
       "      <td>0.176502</td>\n",
       "      <td>0.158375</td>\n",
       "      <td>2020-09-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3177 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp          high           low     volume  \\\n",
       "2012-01-01  1.325427e+09      4.806667      4.806667   7.200667   \n",
       "2012-01-02  1.325535e+09      5.000000      5.000000  19.048000   \n",
       "2012-01-03  1.325605e+09      5.252500      5.252500  11.004660   \n",
       "2012-01-04  1.325682e+09      5.223333      5.200000  11.914807   \n",
       "2012-01-05  1.325771e+09      6.286190      6.281429   4.514373   \n",
       "...                  ...           ...           ...        ...   \n",
       "2020-09-10  1.599739e+09  10343.558552  10334.291588   7.119214   \n",
       "2020-09-11  1.599826e+09  10306.071361  10299.283936   3.660672   \n",
       "2020-09-12  1.599912e+09  10375.469839  10370.388713   1.754713   \n",
       "2020-09-13  1.599999e+09  10396.386007  10390.076996   3.056612   \n",
       "2020-09-14  1.600042e+09  10338.820000  10332.370000   1.292006   \n",
       "\n",
       "            weightedPrice          Open         Close  timestampINT  \\\n",
       "2012-01-01       4.806667      4.806667      4.806667  1.325427e+09   \n",
       "2012-01-02       5.000000      5.000000      5.000000  1.325535e+09   \n",
       "2012-01-03       5.252500      5.252500      5.252500  1.325605e+09   \n",
       "2012-01-04       5.208159      5.200000      5.223333  1.325682e+09   \n",
       "2012-01-05       6.284127      6.281429      6.286190  1.325771e+09   \n",
       "...                   ...           ...           ...           ...   \n",
       "2020-09-10   10339.359896  10338.759708  10338.760864  1.599739e+09   \n",
       "2020-09-11   10303.242947  10302.414327  10302.451926  1.599826e+09   \n",
       "2020-09-12   10373.259995  10372.770434  10372.583252  1.599912e+09   \n",
       "2020-09-13   10393.662602  10393.230099  10392.960134  1.599999e+09   \n",
       "2020-09-14   10332.429402  10338.820000  10332.370000  1.600042e+09   \n",
       "\n",
       "           normalised_date     score       neg       neu       pos  compound  \\\n",
       "2012-01-01      2012-01-01  3.333333  0.046400  0.893367  0.060233  0.086240   \n",
       "2012-01-02      2012-01-02  2.121951  0.079707  0.818707  0.101561  0.042127   \n",
       "2012-01-03      2012-01-03  2.590909  0.080000  0.817523  0.102477  0.029139   \n",
       "2012-01-04      2012-01-04  2.725000  0.078175  0.833075  0.088750  0.049387   \n",
       "2012-01-05      2012-01-05  3.585366  0.058146  0.846829  0.094976  0.170327   \n",
       "...                    ...       ...       ...       ...       ...       ...   \n",
       "2020-09-10      2020-09-10  1.900000  0.065841  0.835543  0.098622  0.170521   \n",
       "2020-09-11      2020-09-11  1.515260  0.068309  0.821795  0.109898  0.147518   \n",
       "2020-09-12      2020-09-12  3.245926  0.065944  0.828458  0.105636  0.152564   \n",
       "2020-09-13      2020-09-13  3.279534  0.064995  0.831902  0.103080  0.144426   \n",
       "2020-09-14      2020-09-14  2.699083  0.064688  0.832695  0.102602  0.176502   \n",
       "\n",
       "             rolling normalised_date_dt64  \n",
       "2012-01-01  0.121112           2012-01-01  \n",
       "2012-01-02  0.078388           2012-01-02  \n",
       "2012-01-03  0.035952           2012-01-03  \n",
       "2012-01-04  0.051089           2012-01-04  \n",
       "2012-01-05  0.195525           2012-01-05  \n",
       "...              ...                  ...  \n",
       "2020-09-10  0.165506           2020-09-10  \n",
       "2020-09-11  0.141176           2020-09-11  \n",
       "2020-09-12  0.165354           2020-09-12  \n",
       "2020-09-13  0.145943           2020-09-13  \n",
       "2020-09-14  0.158375           2020-09-14  \n",
       "\n",
       "[3177 rows x 16 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "6T30wtq9dTgk"
   },
   "outputs": [],
   "source": [
    "date_price_df = aggregated_df[['weightedPrice']]\n",
    "daily_deltas = date_price_df.pct_change(periods=1)\n",
    "# monthly_deltas = date_price_df.pct_change(freq='M')\n",
    "#aggregated_df['pct_change'] = aggregated_df.pct_change('weightedPrice', periods=1, index='normalised_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "id": "wyhgRX62dTgk",
    "outputId": "cabdc09e-55bc-40c5-ab0d-7d2f2535a858",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>daily_pct_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01</th>\n",
       "      <td>4.806667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-02</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.040222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-03</th>\n",
       "      <td>5.252500</td>\n",
       "      <td>0.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04</th>\n",
       "      <td>5.208159</td>\n",
       "      <td>-0.008442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-05</th>\n",
       "      <td>6.284127</td>\n",
       "      <td>0.206593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-10</th>\n",
       "      <td>10339.359896</td>\n",
       "      <td>0.014840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-11</th>\n",
       "      <td>10303.242947</td>\n",
       "      <td>-0.003493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-12</th>\n",
       "      <td>10373.259995</td>\n",
       "      <td>0.006796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-13</th>\n",
       "      <td>10393.662602</td>\n",
       "      <td>0.001967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-14</th>\n",
       "      <td>10332.429402</td>\n",
       "      <td>-0.005891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3177 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            weightedPrice  daily_pct_change\n",
       "2012-01-01       4.806667               NaN\n",
       "2012-01-02       5.000000          0.040222\n",
       "2012-01-03       5.252500          0.050500\n",
       "2012-01-04       5.208159         -0.008442\n",
       "2012-01-05       6.284127          0.206593\n",
       "...                   ...               ...\n",
       "2020-09-10   10339.359896          0.014840\n",
       "2020-09-11   10303.242947         -0.003493\n",
       "2020-09-12   10373.259995          0.006796\n",
       "2020-09-13   10393.662602          0.001967\n",
       "2020-09-14   10332.429402         -0.005891\n",
       "\n",
       "[3177 rows x 2 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_price_df['daily_pct_change'] = daily_deltas\n",
    "date_price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "koGGTYrLdTgl",
    "outputId": "1e982291-f6c0-413c-d397-e9614f8b1fcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3177"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(date_price_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bV2pFTFtdTgl",
    "outputId": "37618309-e3bc-40b4-91a3-e88fae02c583"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2012-01-01', '2012-01-02', '2012-01-03', '2012-01-04',\n",
       "               '2012-01-05', '2012-01-06', '2012-01-07', '2012-01-08',\n",
       "               '2012-01-09', '2012-01-10',\n",
       "               ...\n",
       "               '2020-09-02', '2020-09-03', '2020-09-04', '2020-09-05',\n",
       "               '2020-09-06', '2020-09-07', '2020-09-08', '2020-09-09',\n",
       "               '2020-09-10', '2020-09-11'],\n",
       "              dtype='datetime64[ns]', length=3177, freq='D')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the time-series index\n",
    "n=len(date_price_df)\n",
    "index = pd.date_range('2012-01-01', periods = n,freq='D')\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "oJ9nsq_9dTgm"
   },
   "outputs": [],
   "source": [
    "date_price_df = date_price_df.set_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "OYAmEar-dTgn",
    "outputId": "657e48a4-56d9-463c-a710-711efe063f6f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>monthly_pct_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-31</th>\n",
       "      <td>5.614045</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-02-29</th>\n",
       "      <td>4.952722</td>\n",
       "      <td>-0.117798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-30</th>\n",
       "      <td>4.836740</td>\n",
       "      <td>-0.023418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-30</th>\n",
       "      <td>4.950282</td>\n",
       "      <td>0.023475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-31</th>\n",
       "      <td>5.136571</td>\n",
       "      <td>0.037632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-29</th>\n",
       "      <td>9585.242823</td>\n",
       "      <td>0.067412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30</th>\n",
       "      <td>9093.863232</td>\n",
       "      <td>-0.051264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31</th>\n",
       "      <td>11255.501070</td>\n",
       "      <td>0.237703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-31</th>\n",
       "      <td>11039.063579</td>\n",
       "      <td>-0.019229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>10332.429402</td>\n",
       "      <td>-0.064012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            weightedPrice  monthly_pct_change\n",
       "2012-01-31       5.614045                 NaN\n",
       "2012-02-29       4.952722           -0.117798\n",
       "2012-03-30       4.836740           -0.023418\n",
       "2012-04-30       4.950282            0.023475\n",
       "2012-05-31       5.136571            0.037632\n",
       "...                   ...                 ...\n",
       "2020-05-29    9585.242823            0.067412\n",
       "2020-06-30    9093.863232           -0.051264\n",
       "2020-07-31   11255.501070            0.237703\n",
       "2020-08-31   11039.063579           -0.019229\n",
       "2020-09-30   10332.429402           -0.064012\n",
       "\n",
       "[105 rows x 2 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding monthly % change\n",
    "monthly = date_price_df.resample('BM').apply(lambda x: x[-1])\n",
    "monthly.drop(columns=['daily_pct_change'], inplace=True)\n",
    "monthly['monthly_pct_change'] = monthly.pct_change()\n",
    "monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0Wb52AeYdTgo",
    "outputId": "08f33405-0ba6-4054-f35f-9724a7c87ad1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>quarterly_pct_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-31</th>\n",
       "      <td>5.614045</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-30</th>\n",
       "      <td>4.950282</td>\n",
       "      <td>-0.118233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-31</th>\n",
       "      <td>9.199746</td>\n",
       "      <td>0.858429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10-31</th>\n",
       "      <td>10.826328</td>\n",
       "      <td>0.176807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-31</th>\n",
       "      <td>20.230478</td>\n",
       "      <td>0.868637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-30</th>\n",
       "      <td>141.614016</td>\n",
       "      <td>6.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-31</th>\n",
       "      <td>98.923727</td>\n",
       "      <td>-0.301455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-10-31</th>\n",
       "      <td>200.809802</td>\n",
       "      <td>1.029946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-31</th>\n",
       "      <td>801.904549</td>\n",
       "      <td>2.993354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-30</th>\n",
       "      <td>445.661972</td>\n",
       "      <td>-0.444246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-31</th>\n",
       "      <td>575.249937</td>\n",
       "      <td>0.290776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>342.494621</td>\n",
       "      <td>-0.404616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-31</th>\n",
       "      <td>236.696020</td>\n",
       "      <td>-0.308906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30</th>\n",
       "      <td>238.706066</td>\n",
       "      <td>0.008492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>282.993034</td>\n",
       "      <td>0.185529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-31</th>\n",
       "      <td>384.667023</td>\n",
       "      <td>0.359281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>371.024995</td>\n",
       "      <td>-0.035465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>447.348360</td>\n",
       "      <td>0.205709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>549.336345</td>\n",
       "      <td>0.227983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>720.372995</td>\n",
       "      <td>0.311351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>1011.628648</td>\n",
       "      <td>0.404312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>1472.360516</td>\n",
       "      <td>0.455436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>2743.787179</td>\n",
       "      <td>0.863529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>7240.285580</td>\n",
       "      <td>1.638793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>8910.514730</td>\n",
       "      <td>0.230686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>9379.426678</td>\n",
       "      <td>0.052625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>7393.809379</td>\n",
       "      <td>-0.211699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>6333.015924</td>\n",
       "      <td>-0.143470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>3424.824032</td>\n",
       "      <td>-0.459211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>5618.799695</td>\n",
       "      <td>0.640610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>10784.832145</td>\n",
       "      <td>0.919419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>9217.614909</td>\n",
       "      <td>-0.145317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-31</th>\n",
       "      <td>9330.400381</td>\n",
       "      <td>0.012236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-30</th>\n",
       "      <td>8979.892589</td>\n",
       "      <td>-0.037566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-31</th>\n",
       "      <td>11255.501070</td>\n",
       "      <td>0.253412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-31</th>\n",
       "      <td>10332.429402</td>\n",
       "      <td>-0.082011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            weightedPrice  quarterly_pct_change\n",
       "2012-01-31       5.614045                   NaN\n",
       "2012-04-30       4.950282             -0.118233\n",
       "2012-07-31       9.199746              0.858429\n",
       "2012-10-31      10.826328              0.176807\n",
       "2013-01-31      20.230478              0.868637\n",
       "2013-04-30     141.614016              6.000033\n",
       "2013-07-31      98.923727             -0.301455\n",
       "2013-10-31     200.809802              1.029946\n",
       "2014-01-31     801.904549              2.993354\n",
       "2014-04-30     445.661972             -0.444246\n",
       "2014-07-31     575.249937              0.290776\n",
       "2014-10-31     342.494621             -0.404616\n",
       "2015-01-31     236.696020             -0.308906\n",
       "2015-04-30     238.706066              0.008492\n",
       "2015-07-31     282.993034              0.185529\n",
       "2015-10-31     384.667023              0.359281\n",
       "2016-01-31     371.024995             -0.035465\n",
       "2016-04-30     447.348360              0.205709\n",
       "2016-07-31     549.336345              0.227983\n",
       "2016-10-31     720.372995              0.311351\n",
       "2017-01-31    1011.628648              0.404312\n",
       "2017-04-30    1472.360516              0.455436\n",
       "2017-07-31    2743.787179              0.863529\n",
       "2017-10-31    7240.285580              1.638793\n",
       "2018-01-31    8910.514730              0.230686\n",
       "2018-04-30    9379.426678              0.052625\n",
       "2018-07-31    7393.809379             -0.211699\n",
       "2018-10-31    6333.015924             -0.143470\n",
       "2019-01-31    3424.824032             -0.459211\n",
       "2019-04-30    5618.799695              0.640610\n",
       "2019-07-31   10784.832145              0.919419\n",
       "2019-10-31    9217.614909             -0.145317\n",
       "2020-01-31    9330.400381              0.012236\n",
       "2020-04-30    8979.892589             -0.037566\n",
       "2020-07-31   11255.501070              0.253412\n",
       "2020-10-31   10332.429402             -0.082011"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_months= date_price_df.resample('3M').apply(lambda x: x[-1])\n",
    "three_months.drop(columns=['daily_pct_change'], inplace=True)\n",
    "#three_months['quarterly_pct_change'] = three_months['daily_pct_change']\n",
    "\n",
    "three_months['quarterly_pct_change'] = three_months.pct_change()\n",
    "three_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "TEyWw3fbdTgo",
    "outputId": "32e440d7-0bd6-4d78-cc9e-0efa569db05a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>yearly_pct_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-12-31</th>\n",
       "      <td>13.196513</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-31</th>\n",
       "      <td>728.728912</td>\n",
       "      <td>54.221323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31</th>\n",
       "      <td>314.514811</td>\n",
       "      <td>-0.568406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>429.069676</td>\n",
       "      <td>0.364227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>1019.992995</td>\n",
       "      <td>1.377220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>14962.379078</td>\n",
       "      <td>13.669100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>3829.104391</td>\n",
       "      <td>-0.744085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>7218.040867</td>\n",
       "      <td>0.885047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31</th>\n",
       "      <td>10332.429402</td>\n",
       "      <td>0.431473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            weightedPrice  yearly_pct_change\n",
       "2012-12-31      13.196513                NaN\n",
       "2013-12-31     728.728912          54.221323\n",
       "2014-12-31     314.514811          -0.568406\n",
       "2015-12-31     429.069676           0.364227\n",
       "2016-12-31    1019.992995           1.377220\n",
       "2017-12-31   14962.379078          13.669100\n",
       "2018-12-31    3829.104391          -0.744085\n",
       "2019-12-31    7218.040867           0.885047\n",
       "2020-12-31   10332.429402           0.431473"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yearly = date_price_df.resample('Y').apply(lambda x: x[-1])\n",
    "yearly.drop(columns=['daily_pct_change'], inplace=True)\n",
    "# yearly['yearly_pct_change'] = yearly['daily_pct_change']\n",
    "yearly['yearly_pct_change'] = yearly.pct_change()\n",
    "yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "bp2TyJEsdTgp",
    "outputId": "f8ddacc0-216e-484f-c0db-4c789cc07214"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>daily_pct_change</th>\n",
       "      <th>monthly_pct_change</th>\n",
       "      <th>dt_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.806667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.040222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.252500</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.208159</td>\n",
       "      <td>-0.008442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.284127</td>\n",
       "      <td>0.206593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>10339.359896</td>\n",
       "      <td>0.014840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3173</th>\n",
       "      <td>10303.242947</td>\n",
       "      <td>-0.003493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>10373.259995</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>10393.662602</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>10332.429402</td>\n",
       "      <td>-0.005891</td>\n",
       "      <td>-0.064012</td>\n",
       "      <td>2020-09-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3177 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      weightedPrice  daily_pct_change  monthly_pct_change   dt_index\n",
       "0          4.806667               NaN                 NaN 2012-01-01\n",
       "1          5.000000          0.040222                 NaN 2012-01-02\n",
       "2          5.252500          0.050500                 NaN 2012-01-03\n",
       "3          5.208159         -0.008442                 NaN 2012-01-04\n",
       "4          6.284127          0.206593                 NaN 2012-01-05\n",
       "...             ...               ...                 ...        ...\n",
       "3172   10339.359896          0.014840                 NaN 2020-09-07\n",
       "3173   10303.242947         -0.003493                 NaN 2020-09-08\n",
       "3174   10373.259995          0.006796                 NaN 2020-09-09\n",
       "3175   10393.662602          0.001967                 NaN 2020-09-10\n",
       "3176   10332.429402         -0.005891           -0.064012 2020-09-11\n",
       "\n",
       "[3177 rows x 4 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_price_merged = pd.merge(left=date_price_df, right=monthly, how='left')\n",
    "date_price_merged['dt_index'] = index\n",
    "date_price_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "JkOqWp2zdTgq",
    "outputId": "13c58900-4e38-4cf5-9176-96127f52260f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>daily_pct_change</th>\n",
       "      <th>monthly_pct_change</th>\n",
       "      <th>dt_index</th>\n",
       "      <th>yearly_pct_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.806667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.040222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.252500</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.208159</td>\n",
       "      <td>-0.008442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.284127</td>\n",
       "      <td>0.206593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>10339.359896</td>\n",
       "      <td>0.014840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3173</th>\n",
       "      <td>10303.242947</td>\n",
       "      <td>-0.003493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-08</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>10373.259995</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>10393.662602</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>10332.429402</td>\n",
       "      <td>-0.005891</td>\n",
       "      <td>-0.064012</td>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>0.431473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3177 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      weightedPrice  daily_pct_change  monthly_pct_change   dt_index  \\\n",
       "0          4.806667               NaN                 NaN 2012-01-01   \n",
       "1          5.000000          0.040222                 NaN 2012-01-02   \n",
       "2          5.252500          0.050500                 NaN 2012-01-03   \n",
       "3          5.208159         -0.008442                 NaN 2012-01-04   \n",
       "4          6.284127          0.206593                 NaN 2012-01-05   \n",
       "...             ...               ...                 ...        ...   \n",
       "3172   10339.359896          0.014840                 NaN 2020-09-07   \n",
       "3173   10303.242947         -0.003493                 NaN 2020-09-08   \n",
       "3174   10373.259995          0.006796                 NaN 2020-09-09   \n",
       "3175   10393.662602          0.001967                 NaN 2020-09-10   \n",
       "3176   10332.429402         -0.005891           -0.064012 2020-09-11   \n",
       "\n",
       "      yearly_pct_change  \n",
       "0                   NaN  \n",
       "1                   NaN  \n",
       "2                   NaN  \n",
       "3                   NaN  \n",
       "4                   NaN  \n",
       "...                 ...  \n",
       "3172                NaN  \n",
       "3173                NaN  \n",
       "3174                NaN  \n",
       "3175                NaN  \n",
       "3176           0.431473  \n",
       "\n",
       "[3177 rows x 5 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_price_merged = pd.merge(left=date_price_merged, right=yearly, how='left')\n",
    "date_price_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_bUSpRudTgq",
    "outputId": "a5adc37a-4b8c-47ab-acd5-66e225d15605"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weightedPrice                     4.95272\n",
       "daily_pct_change              -0.00779485\n",
       "monthly_pct_change              -0.117798\n",
       "dt_index              2012-02-29 00:00:00\n",
       "yearly_pct_change                     NaN\n",
       "Name: 59, dtype: object"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_price_merged.iloc[59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "PnxAPiZmdTgr"
   },
   "outputs": [],
   "source": [
    "log_diffs = aggregated_df[['Open','Close']]\n",
    "# date_price_merged['daily_log_diff'] = np.log(log_diffs['Close']) - np.log(log_diffs['Open'])\n",
    "date_price_merged['daily_log_diff'] = [np.log(x)- np.log(y) for x,y in zip(log_diffs['Close'], log_diffs['Open'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "ha39aKLWdTgs",
    "outputId": "96b47987-9287-43c1-eb66-8709d5f2665b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>daily_pct_change</th>\n",
       "      <th>monthly_pct_change</th>\n",
       "      <th>dt_index</th>\n",
       "      <th>yearly_pct_change</th>\n",
       "      <th>daily_log_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.806667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.040222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.252500</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.208159</td>\n",
       "      <td>-0.008442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.477142e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.284127</td>\n",
       "      <td>0.206593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.578054e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>10339.359896</td>\n",
       "      <td>0.014840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.118112e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3173</th>\n",
       "      <td>10303.242947</td>\n",
       "      <td>-0.003493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.649569e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>10373.259995</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.804566e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>10393.662602</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.597539e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>10332.429402</td>\n",
       "      <td>-0.005891</td>\n",
       "      <td>-0.064012</td>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>0.431473</td>\n",
       "      <td>-6.240570e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3177 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      weightedPrice  daily_pct_change  monthly_pct_change   dt_index  \\\n",
       "0          4.806667               NaN                 NaN 2012-01-01   \n",
       "1          5.000000          0.040222                 NaN 2012-01-02   \n",
       "2          5.252500          0.050500                 NaN 2012-01-03   \n",
       "3          5.208159         -0.008442                 NaN 2012-01-04   \n",
       "4          6.284127          0.206593                 NaN 2012-01-05   \n",
       "...             ...               ...                 ...        ...   \n",
       "3172   10339.359896          0.014840                 NaN 2020-09-07   \n",
       "3173   10303.242947         -0.003493                 NaN 2020-09-08   \n",
       "3174   10373.259995          0.006796                 NaN 2020-09-09   \n",
       "3175   10393.662602          0.001967                 NaN 2020-09-10   \n",
       "3176   10332.429402         -0.005891           -0.064012 2020-09-11   \n",
       "\n",
       "      yearly_pct_change  daily_log_diff  \n",
       "0                   NaN    0.000000e+00  \n",
       "1                   NaN    0.000000e+00  \n",
       "2                   NaN    0.000000e+00  \n",
       "3                   NaN    4.477142e-03  \n",
       "4                   NaN    7.578054e-04  \n",
       "...                 ...             ...  \n",
       "3172                NaN    1.118112e-07  \n",
       "3173                NaN    3.649569e-06  \n",
       "3174                NaN   -1.804566e-05  \n",
       "3175                NaN   -2.597539e-05  \n",
       "3176           0.431473   -6.240570e-04  \n",
       "\n",
       "[3177 rows x 6 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log diff can be interpreted as an approximation of \"percentage change\"\n",
    "date_price_merged\n",
    "\n",
    "# for visualization purposes, might be interesting to plot normal weightedPrice vs Time, daily_log_diff vs Time, and a log_diff histogram\n",
    "# when presenting, can filter by month/year to better observe changes within each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "N8mFFVizdTgs",
    "outputId": "c6d1857f-d712-4a06-d5ab-c6faf0943b33"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>daily_pct_change</th>\n",
       "      <th>monthly_pct_change</th>\n",
       "      <th>dt_index</th>\n",
       "      <th>yearly_pct_change</th>\n",
       "      <th>daily_log_diff</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.806667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.040222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.252500</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.208159</td>\n",
       "      <td>-0.008442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.477142e-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.284127</td>\n",
       "      <td>0.206593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.578054e-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>10339.359896</td>\n",
       "      <td>0.014840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.118112e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3173</th>\n",
       "      <td>10303.242947</td>\n",
       "      <td>-0.003493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.649569e-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>10373.259995</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.804566e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>10393.662602</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.597539e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>10332.429402</td>\n",
       "      <td>-0.005891</td>\n",
       "      <td>-0.064012</td>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>0.431473</td>\n",
       "      <td>-6.240570e-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3177 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      weightedPrice  daily_pct_change  monthly_pct_change   dt_index  \\\n",
       "0          4.806667               NaN                 NaN 2012-01-01   \n",
       "1          5.000000          0.040222                 NaN 2012-01-02   \n",
       "2          5.252500          0.050500                 NaN 2012-01-03   \n",
       "3          5.208159         -0.008442                 NaN 2012-01-04   \n",
       "4          6.284127          0.206593                 NaN 2012-01-05   \n",
       "...             ...               ...                 ...        ...   \n",
       "3172   10339.359896          0.014840                 NaN 2020-09-07   \n",
       "3173   10303.242947         -0.003493                 NaN 2020-09-08   \n",
       "3174   10373.259995          0.006796                 NaN 2020-09-09   \n",
       "3175   10393.662602          0.001967                 NaN 2020-09-10   \n",
       "3176   10332.429402         -0.005891           -0.064012 2020-09-11   \n",
       "\n",
       "      yearly_pct_change  daily_log_diff  target  \n",
       "0                   NaN    0.000000e+00       0  \n",
       "1                   NaN    0.000000e+00       0  \n",
       "2                   NaN    0.000000e+00       0  \n",
       "3                   NaN    4.477142e-03       1  \n",
       "4                   NaN    7.578054e-04       1  \n",
       "...                 ...             ...     ...  \n",
       "3172                NaN    1.118112e-07       1  \n",
       "3173                NaN    3.649569e-06       1  \n",
       "3174                NaN   -1.804566e-05       0  \n",
       "3175                NaN   -2.597539e-05       0  \n",
       "3176           0.431473   -6.240570e-04       0  \n",
       "\n",
       "[3177 rows x 7 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating our targets (1 for net pct increase, 0 for no change/decrease)\n",
    "# our target predicts whether or not there is positive performance\n",
    "date_price_merged['target'] = [1 if x > 0 else 0 for x in date_price_merged['daily_log_diff']]\n",
    "date_price_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "4Kh6_rradTgs"
   },
   "outputs": [],
   "source": [
    "# can use OpenBlender (NLP) to supplement findings with news articles, vectorize input parameters as 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "rACKaeX9dTgt"
   },
   "outputs": [],
   "source": [
    "# applying ML model and observing results\n",
    "# importing dependencies\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "uIB8KS-IdTgu",
    "outputId": "807d89e0-d028-4134-ea31-cd65b098edd5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>daily_pct_change</th>\n",
       "      <th>dt_index</th>\n",
       "      <th>daily_log_diff</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.040222</td>\n",
       "      <td>2012-01-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.252500</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.208159</td>\n",
       "      <td>-0.008442</td>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>4.477142e-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.284127</td>\n",
       "      <td>0.206593</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>7.578054e-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.438999</td>\n",
       "      <td>0.024645</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>1.552795e-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>10339.359896</td>\n",
       "      <td>0.014840</td>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>1.118112e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3173</th>\n",
       "      <td>10303.242947</td>\n",
       "      <td>-0.003493</td>\n",
       "      <td>2020-09-08</td>\n",
       "      <td>3.649569e-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>10373.259995</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>-1.804566e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>10393.662602</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>-2.597539e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>10332.429402</td>\n",
       "      <td>-0.005891</td>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>-6.240570e-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3176 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      weightedPrice  daily_pct_change   dt_index  daily_log_diff  target\n",
       "1          5.000000          0.040222 2012-01-02    0.000000e+00       0\n",
       "2          5.252500          0.050500 2012-01-03    0.000000e+00       0\n",
       "3          5.208159         -0.008442 2012-01-04    4.477142e-03       1\n",
       "4          6.284127          0.206593 2012-01-05    7.578054e-04       1\n",
       "5          6.438999          0.024645 2012-01-06    1.552795e-03       1\n",
       "...             ...               ...        ...             ...     ...\n",
       "3172   10339.359896          0.014840 2020-09-07    1.118112e-07       1\n",
       "3173   10303.242947         -0.003493 2020-09-08    3.649569e-06       1\n",
       "3174   10373.259995          0.006796 2020-09-09   -1.804566e-05       0\n",
       "3175   10393.662602          0.001967 2020-09-10   -2.597539e-05       0\n",
       "3176   10332.429402         -0.005891 2020-09-11   -6.240570e-04       0\n",
       "\n",
       "[3176 rows x 5 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temporarily dropping monthly/annual log calculations since they contain NaN values\n",
    "# also dropping very 1st value (cannot calculate deltas from nothing)\n",
    "date_price_merged2 = date_price_merged.drop(columns=['monthly_pct_change','yearly_pct_change'])\n",
    "date_price_merged2 = date_price_merged2.iloc[1:]\n",
    "date_price_merged2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>daily_pct_change</th>\n",
       "      <th>dt_index</th>\n",
       "      <th>daily_log_diff</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-02</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.040222</td>\n",
       "      <td>2012-01-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-03</th>\n",
       "      <td>5.252500</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04</th>\n",
       "      <td>5.208159</td>\n",
       "      <td>-0.008442</td>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>4.477142e-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-05</th>\n",
       "      <td>6.284127</td>\n",
       "      <td>0.206593</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>7.578054e-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-06</th>\n",
       "      <td>6.438999</td>\n",
       "      <td>0.024645</td>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>1.552795e-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-07</th>\n",
       "      <td>10339.359896</td>\n",
       "      <td>0.014840</td>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>1.118112e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-08</th>\n",
       "      <td>10303.242947</td>\n",
       "      <td>-0.003493</td>\n",
       "      <td>2020-09-08</td>\n",
       "      <td>3.649569e-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-09</th>\n",
       "      <td>10373.259995</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>2020-09-09</td>\n",
       "      <td>-1.804566e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-10</th>\n",
       "      <td>10393.662602</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>2020-09-10</td>\n",
       "      <td>-2.597539e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-11</th>\n",
       "      <td>10332.429402</td>\n",
       "      <td>-0.005891</td>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>-6.240570e-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3176 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            weightedPrice  daily_pct_change   dt_index  daily_log_diff  target\n",
       "dt_index                                                                      \n",
       "2012-01-02       5.000000          0.040222 2012-01-02    0.000000e+00       0\n",
       "2012-01-03       5.252500          0.050500 2012-01-03    0.000000e+00       0\n",
       "2012-01-04       5.208159         -0.008442 2012-01-04    4.477142e-03       1\n",
       "2012-01-05       6.284127          0.206593 2012-01-05    7.578054e-04       1\n",
       "2012-01-06       6.438999          0.024645 2012-01-06    1.552795e-03       1\n",
       "...                   ...               ...        ...             ...     ...\n",
       "2020-09-07   10339.359896          0.014840 2020-09-07    1.118112e-07       1\n",
       "2020-09-08   10303.242947         -0.003493 2020-09-08    3.649569e-06       1\n",
       "2020-09-09   10373.259995          0.006796 2020-09-09   -1.804566e-05       0\n",
       "2020-09-10   10393.662602          0.001967 2020-09-10   -2.597539e-05       0\n",
       "2020-09-11   10332.429402         -0.005891 2020-09-11   -6.240570e-04       0\n",
       "\n",
       "[3176 rows x 5 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_price_merged3 = date_price_merged2.set_index(date_price_merged2['dt_index'])\n",
    "date_price_merged3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['weightedPrice', 'daily_pct_change', 'dt_index', 'daily_log_diff',\n",
       "       'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_price_merged2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>daily_pct_change</th>\n",
       "      <th>target</th>\n",
       "      <th>score</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-02</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.040222</td>\n",
       "      <td>0</td>\n",
       "      <td>2.121951</td>\n",
       "      <td>0.079707</td>\n",
       "      <td>0.818707</td>\n",
       "      <td>0.101561</td>\n",
       "      <td>0.042127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-03</th>\n",
       "      <td>5.252500</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0</td>\n",
       "      <td>2.590909</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.817523</td>\n",
       "      <td>0.102477</td>\n",
       "      <td>0.029139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04</th>\n",
       "      <td>5.208159</td>\n",
       "      <td>-0.008442</td>\n",
       "      <td>1</td>\n",
       "      <td>2.725000</td>\n",
       "      <td>0.078175</td>\n",
       "      <td>0.833075</td>\n",
       "      <td>0.088750</td>\n",
       "      <td>0.049387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-05</th>\n",
       "      <td>6.284127</td>\n",
       "      <td>0.206593</td>\n",
       "      <td>1</td>\n",
       "      <td>3.585366</td>\n",
       "      <td>0.058146</td>\n",
       "      <td>0.846829</td>\n",
       "      <td>0.094976</td>\n",
       "      <td>0.170327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-06</th>\n",
       "      <td>6.438999</td>\n",
       "      <td>0.024645</td>\n",
       "      <td>1</td>\n",
       "      <td>2.063830</td>\n",
       "      <td>0.063638</td>\n",
       "      <td>0.825553</td>\n",
       "      <td>0.110809</td>\n",
       "      <td>0.210717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-07</th>\n",
       "      <td>10339.359896</td>\n",
       "      <td>0.014840</td>\n",
       "      <td>1</td>\n",
       "      <td>1.257449</td>\n",
       "      <td>0.068073</td>\n",
       "      <td>0.825540</td>\n",
       "      <td>0.106390</td>\n",
       "      <td>0.156567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-08</th>\n",
       "      <td>10303.242947</td>\n",
       "      <td>-0.003493</td>\n",
       "      <td>1</td>\n",
       "      <td>1.080357</td>\n",
       "      <td>0.067413</td>\n",
       "      <td>0.823720</td>\n",
       "      <td>0.108853</td>\n",
       "      <td>0.185623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-09</th>\n",
       "      <td>10373.259995</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>0</td>\n",
       "      <td>1.156284</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.831169</td>\n",
       "      <td>0.103004</td>\n",
       "      <td>0.156549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-10</th>\n",
       "      <td>10393.662602</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.065841</td>\n",
       "      <td>0.835543</td>\n",
       "      <td>0.098622</td>\n",
       "      <td>0.170521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-11</th>\n",
       "      <td>10332.429402</td>\n",
       "      <td>-0.005891</td>\n",
       "      <td>0</td>\n",
       "      <td>1.515260</td>\n",
       "      <td>0.068309</td>\n",
       "      <td>0.821795</td>\n",
       "      <td>0.109898</td>\n",
       "      <td>0.147518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3176 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            weightedPrice  daily_pct_change  target     score       neg  \\\n",
       "2012-01-02       5.000000          0.040222       0  2.121951  0.079707   \n",
       "2012-01-03       5.252500          0.050500       0  2.590909  0.080000   \n",
       "2012-01-04       5.208159         -0.008442       1  2.725000  0.078175   \n",
       "2012-01-05       6.284127          0.206593       1  3.585366  0.058146   \n",
       "2012-01-06       6.438999          0.024645       1  2.063830  0.063638   \n",
       "...                   ...               ...     ...       ...       ...   \n",
       "2020-09-07   10339.359896          0.014840       1  1.257449  0.068073   \n",
       "2020-09-08   10303.242947         -0.003493       1  1.080357  0.067413   \n",
       "2020-09-09   10373.259995          0.006796       0  1.156284  0.065800   \n",
       "2020-09-10   10393.662602          0.001967       0  1.900000  0.065841   \n",
       "2020-09-11   10332.429402         -0.005891       0  1.515260  0.068309   \n",
       "\n",
       "                 neu       pos  compound  \n",
       "2012-01-02  0.818707  0.101561  0.042127  \n",
       "2012-01-03  0.817523  0.102477  0.029139  \n",
       "2012-01-04  0.833075  0.088750  0.049387  \n",
       "2012-01-05  0.846829  0.094976  0.170327  \n",
       "2012-01-06  0.825553  0.110809  0.210717  \n",
       "...              ...       ...       ...  \n",
       "2020-09-07  0.825540  0.106390  0.156567  \n",
       "2020-09-08  0.823720  0.108853  0.185623  \n",
       "2020-09-09  0.831169  0.103004  0.156549  \n",
       "2020-09-10  0.835543  0.098622  0.170521  \n",
       "2020-09-11  0.821795  0.109898  0.147518  \n",
       "\n",
       "[3176 rows x 8 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_price_merged2 = pd.concat([date_price_merged3, sentiment_scores_df2], axis=1, join='inner')\n",
    "date_price_merged2.drop(columns=['normalised_date','daily_log_diff','rolling','dt_index'], inplace=True)\n",
    "date_price_merged2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "bAFmIh7QdTgv"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# creating training and testing datasets\n",
    "df = date_price_merged2\n",
    "\n",
    "X = df.loc[:,df.columns != 'target'].select_dtypes(include=[np.number]).values\n",
    "y = df.loc[:,['target']].values\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(X)\n",
    "# div = int(round(len(X) * 0.2)) # allocating 20% of dataset for testing\n",
    "\n",
    "# shuffling dataset to create randomized test and train splits\n",
    "# x = np.random.rand(len(df),5)\n",
    "# np.random.shuffle(x)\n",
    "# training, test = x[:div,:], x[div:,:]\n",
    "\n",
    "# X_train = X[:div]\n",
    "# y_train = y[:div]\n",
    "# X_test = X[div:]\n",
    "# y_test= y[div:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2540\n",
      "636\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "E7he6qVtdTgw",
    "outputId": "f0237ab0-ded3-4cb9-fe59-870e9581d7a3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>1</td>\n",
       "      <td>0.307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>0</td>\n",
       "      <td>0.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>0</td>\n",
       "      <td>0.306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>1</td>\n",
       "      <td>0.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>0</td>\n",
       "      <td>0.346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>636 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_test  y_pred\n",
       "0         0   0.567\n",
       "1         1   0.606\n",
       "2         0   0.578\n",
       "3         1   0.695\n",
       "4         0   0.502\n",
       "..      ...     ...\n",
       "631       1   0.307\n",
       "632       0   0.360\n",
       "633       0   0.306\n",
       "634       1   0.670\n",
       "635       0   0.346\n",
       "\n",
       "[636 rows x 2 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating random forest classifier\n",
    "cls = RandomForestRegressor(n_estimators=1000)\n",
    "cls.fit(X_train, np.ravel(y_train, order='C'))\n",
    "y_pred = cls.predict(X_test)\n",
    "\n",
    "df_residuals = pd.DataFrame({'y_test':y_test[:, 0], 'y_pred':y_pred})\n",
    "df_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kg4AisqdTgw",
    "outputId": "4db4a74a-aea3-4cb6-9198-c24c71466a5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[182 145]\n",
      " [134 175]]\n",
      "Accuracy score: \n",
      "0.5613207547169812\n",
      "Precision score: \n",
      "0.546875\n"
     ]
    }
   ],
   "source": [
    "# calculating base model's accuracy based on only 'weightedPrice' and 'daily_pct_change'\n",
    "threshold = 0.5\n",
    "predictions = [1 if val > threshold else 0 for val in df_residuals['y_pred']]\n",
    "print(confusion_matrix(predictions, df_residuals['y_test']))\n",
    "\n",
    "print('Accuracy score: ')\n",
    "print(accuracy_score(predictions, df_residuals['y_test']))\n",
    "\n",
    "print('Precision score: ')\n",
    "print(precision_score(predictions, df_residuals['y_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "FmH0xxQTdTgx"
   },
   "outputs": [],
   "source": [
    "# based on the confusion matrix, there were 1329 total daily price increase events\n",
    "# our model predicted 1329 + 3 = 1,332 total price increase events, where 3 were False Positives\n",
    "# there were a total of 1,209 + 3 = 1,212 total price DECREASE events, our model detected 1,209 and missed 3 events\n",
    "\n",
    "# precision: high precision indicates low false positive rate, also known as ratio of correctly predicted positive observations to all observations in total class\n",
    "# accuracy: high accuracy indicates the ratio of correct predictions made out of total observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "zXDX7rNsdTgx"
   },
   "outputs": [],
   "source": [
    "# comparing multiple models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [20, 220, 420, 620, 820, 1020, 1220, 1420, 1620, 1820, 2020], 'max_features': ['auto', 'sqrt', 'log2', None], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 4, 6, 10], 'min_samples_leaf': [1, 2, 4, 6, 8], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# calculating optimal hyperparameters for random forest classifier based on RandomSearchCV\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "n_estimators = [int(x) for x in range(20,2021,200)] #trees per forest\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "max_depth = [int(x) for x in range(10,111,10)] #tree depth\n",
    "max_depth.append(None)\n",
    "\n",
    "min_samples_split = [2, 4, 6, 10]\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# input grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after generating random input grid, create base model\n",
    "rf_random = RandomizedSearchCV(estimator=RandomForestClassifier(), \n",
    "                    param_distributions=random_grid,\n",
    "                    n_iter=100,\n",
    "                    cv=3,\n",
    "                    refit=True,\n",
    "                    verbose=3,\n",
    "                    error_score=0,\n",
    "                    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  7.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 820,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 6,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 100,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model \n",
    "rf_random.fit(X_train, np.ravel(y_train, order='C'))\n",
    "\n",
    "# viewing the best parameters from cross validation\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=100, max_features='sqrt', min_samples_leaf=6,\n",
       "                       min_samples_split=10, n_estimators=820)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating our random forest classifier with tuned hyperparameters\n",
    "best_random = rf_random.best_estimator_\n",
    "best_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV tuning for XGBoost\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'min_child_weght': [1,5,10],\n",
    "    'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [3,4,5]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic', silent=False)\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "\n",
    "# brute force grid search\n",
    "grid = GridSearchCV(estimator=xgb, \n",
    "                    param_grid=params,\n",
    "                    cv=10,\n",
    "                    verbose=3,\n",
    "                    scoring='roc_auc',\n",
    "                    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 405 candidates, totalling 4050 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   37.1s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 22.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3856 tasks      | elapsed: 27.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4050 out of 4050 | elapsed: 29.5min finished\n",
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:18:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_child_weght, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:18:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "# fiting tuned xgboost\n",
    "fitted_model = grid.fit(X_train, np.ravel(y_train, order='C'))\n",
    "best_xgb = fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "EH0O0rXMfGHf"
   },
   "outputs": [],
   "source": [
    "def compare_models(X_train, y_train, X_test, y_test):\n",
    "  models = [\n",
    "            ('LogReg', LogisticRegression()),\n",
    "            ('RF', RandomForestClassifier()),\n",
    "            ('KNN', KNeighborsClassifier()),\n",
    "            ('SVC', SVC()),\n",
    "            ('GNB', GaussianNB()),\n",
    "            ('XGB', XGBClassifier()),\n",
    "            ('oRF', best_random),\n",
    "            ('oXGB', best_xgb)\n",
    "  ]\n",
    "\n",
    "  dfs = []\n",
    "  results = []\n",
    "  names = []\n",
    "  scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']\n",
    "  target_names = ['increase', 'decrease']\n",
    "\n",
    "\n",
    "# performing K-Fold cross validation \n",
    "  for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=5, shuffle=True)\n",
    "    cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    clf = model.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(name)\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    this_df = pd.DataFrame(cv_results)\n",
    "    this_df['model'] = name\n",
    "    dfs.append(this_df)\n",
    "  \n",
    "  final = pd.concat(dfs, ignore_index=True)\n",
    "  return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SJ1fUrp7jtfa",
    "outputId": "7c1e4f7d-9bfe-4baf-a899-c6cc15f426c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    increase       0.00      0.00      0.00       316\n",
      "    decrease       0.50      1.00      0.67       320\n",
      "\n",
      "    accuracy                           0.50       636\n",
      "   macro avg       0.25      0.50      0.33       636\n",
      "weighted avg       0.25      0.50      0.34       636\n",
      "\n",
      "RF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    increase       0.57      0.58      0.57       316\n",
      "    decrease       0.58      0.57      0.57       320\n",
      "\n",
      "    accuracy                           0.57       636\n",
      "   macro avg       0.57      0.57      0.57       636\n",
      "weighted avg       0.57      0.57      0.57       636\n",
      "\n",
      "KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    increase       0.56      0.57      0.56       316\n",
      "    decrease       0.57      0.56      0.56       320\n",
      "\n",
      "    accuracy                           0.56       636\n",
      "   macro avg       0.56      0.56      0.56       636\n",
      "weighted avg       0.56      0.56      0.56       636\n",
      "\n",
      "SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    increase       0.53      0.60      0.57       316\n",
      "    decrease       0.55      0.48      0.51       320\n",
      "\n",
      "    accuracy                           0.54       636\n",
      "   macro avg       0.54      0.54      0.54       636\n",
      "weighted avg       0.54      0.54      0.54       636\n",
      "\n",
      "GNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    increase       0.52      0.68      0.59       316\n",
      "    decrease       0.55      0.39      0.46       320\n",
      "\n",
      "    accuracy                           0.53       636\n",
      "   macro avg       0.54      0.54      0.53       636\n",
      "weighted avg       0.54      0.53      0.53       636\n",
      "\n",
      "[17:18:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:18:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:18:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:18:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:18:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:18:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    increase       0.53      0.54      0.54       316\n",
      "    decrease       0.54      0.53      0.53       320\n",
      "\n",
      "    accuracy                           0.53       636\n",
      "   macro avg       0.53      0.53      0.53       636\n",
      "weighted avg       0.53      0.53      0.53       636\n",
      "\n",
      "oRF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    increase       0.58      0.58      0.58       316\n",
      "    decrease       0.59      0.59      0.59       320\n",
      "\n",
      "    accuracy                           0.58       636\n",
      "   macro avg       0.58      0.58      0.58       636\n",
      "weighted avg       0.58      0.58      0.58       636\n",
      "\n",
      "Fitting 10 folds for each of 405 candidates, totalling 4050 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   31.0s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 18.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3856 tasks      | elapsed: 22.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4050 out of 4050 | elapsed: 23.6min finished\n",
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:42:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_child_weght, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:42:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 10 folds for each of 405 candidates, totalling 4050 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 17.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3856 tasks      | elapsed: 21.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4050 out of 4050 | elapsed: 22.9min finished\n",
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:05:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_child_weght, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:05:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 10 folds for each of 405 candidates, totalling 4050 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   28.9s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3856 tasks      | elapsed: 28.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4050 out of 4050 | elapsed: 29.7min finished\n",
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:35:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_child_weght, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:35:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 10 folds for each of 405 candidates, totalling 4050 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 16.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3856 tasks      | elapsed: 23.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4050 out of 4050 | elapsed: 24.4min finished\n",
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:59:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_child_weght, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:59:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 10 folds for each of 405 candidates, totalling 4050 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   28.4s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3856 tasks      | elapsed: 20.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4050 out of 4050 | elapsed: 21.6min finished\n",
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:21:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_child_weght, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:21:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 10 folds for each of 405 candidates, totalling 4050 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 20.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3856 tasks      | elapsed: 25.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4050 out of 4050 | elapsed: 26.5min finished\n",
      "C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:47:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { min_child_weght, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:47:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "oXGB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    increase       0.58      0.59      0.58       316\n",
      "    decrease       0.58      0.57      0.58       320\n",
      "\n",
      "    accuracy                           0.58       636\n",
      "   macro avg       0.58      0.58      0.58       636\n",
      "weighted avg       0.58      0.58      0.58       636\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision_weighted</th>\n",
       "      <th>test_recall_weighted</th>\n",
       "      <th>test_f1_weighted</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012955</td>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.509843</td>\n",
       "      <td>0.541768</td>\n",
       "      <td>0.509843</td>\n",
       "      <td>0.492145</td>\n",
       "      <td>0.528148</td>\n",
       "      <td>LogReg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003987</td>\n",
       "      <td>0.004985</td>\n",
       "      <td>0.498031</td>\n",
       "      <td>0.248035</td>\n",
       "      <td>0.498031</td>\n",
       "      <td>0.331148</td>\n",
       "      <td>0.553298</td>\n",
       "      <td>LogReg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010964</td>\n",
       "      <td>0.003986</td>\n",
       "      <td>0.519685</td>\n",
       "      <td>0.515188</td>\n",
       "      <td>0.519685</td>\n",
       "      <td>0.510596</td>\n",
       "      <td>0.516109</td>\n",
       "      <td>LogReg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.004982</td>\n",
       "      <td>0.517717</td>\n",
       "      <td>0.268030</td>\n",
       "      <td>0.517717</td>\n",
       "      <td>0.353202</td>\n",
       "      <td>0.565981</td>\n",
       "      <td>LogReg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.004982</td>\n",
       "      <td>0.478346</td>\n",
       "      <td>0.228815</td>\n",
       "      <td>0.478346</td>\n",
       "      <td>0.309556</td>\n",
       "      <td>0.578803</td>\n",
       "      <td>LogReg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.586042</td>\n",
       "      <td>0.048870</td>\n",
       "      <td>0.505906</td>\n",
       "      <td>0.506085</td>\n",
       "      <td>0.505906</td>\n",
       "      <td>0.505758</td>\n",
       "      <td>0.534714</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.521223</td>\n",
       "      <td>0.040863</td>\n",
       "      <td>0.541339</td>\n",
       "      <td>0.541697</td>\n",
       "      <td>0.541339</td>\n",
       "      <td>0.541045</td>\n",
       "      <td>0.576335</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.540192</td>\n",
       "      <td>0.038906</td>\n",
       "      <td>0.566929</td>\n",
       "      <td>0.567479</td>\n",
       "      <td>0.566929</td>\n",
       "      <td>0.567063</td>\n",
       "      <td>0.586172</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.570056</td>\n",
       "      <td>0.052824</td>\n",
       "      <td>0.547244</td>\n",
       "      <td>0.552169</td>\n",
       "      <td>0.547244</td>\n",
       "      <td>0.547244</td>\n",
       "      <td>0.562988</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.747499</td>\n",
       "      <td>0.055812</td>\n",
       "      <td>0.559055</td>\n",
       "      <td>0.560930</td>\n",
       "      <td>0.559055</td>\n",
       "      <td>0.559438</td>\n",
       "      <td>0.571122</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.003986</td>\n",
       "      <td>0.022967</td>\n",
       "      <td>0.561024</td>\n",
       "      <td>0.560879</td>\n",
       "      <td>0.561024</td>\n",
       "      <td>0.560845</td>\n",
       "      <td>0.565667</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.561024</td>\n",
       "      <td>0.560977</td>\n",
       "      <td>0.561024</td>\n",
       "      <td>0.560214</td>\n",
       "      <td>0.570876</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.021922</td>\n",
       "      <td>0.551181</td>\n",
       "      <td>0.551414</td>\n",
       "      <td>0.551181</td>\n",
       "      <td>0.550261</td>\n",
       "      <td>0.576843</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.021970</td>\n",
       "      <td>0.568898</td>\n",
       "      <td>0.569465</td>\n",
       "      <td>0.568898</td>\n",
       "      <td>0.569073</td>\n",
       "      <td>0.561635</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.021882</td>\n",
       "      <td>0.590551</td>\n",
       "      <td>0.590551</td>\n",
       "      <td>0.590551</td>\n",
       "      <td>0.590551</td>\n",
       "      <td>0.599347</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.149500</td>\n",
       "      <td>0.041860</td>\n",
       "      <td>0.537402</td>\n",
       "      <td>0.549090</td>\n",
       "      <td>0.537402</td>\n",
       "      <td>0.534247</td>\n",
       "      <td>0.539116</td>\n",
       "      <td>SVC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.149498</td>\n",
       "      <td>0.043854</td>\n",
       "      <td>0.574803</td>\n",
       "      <td>0.574395</td>\n",
       "      <td>0.574803</td>\n",
       "      <td>0.571686</td>\n",
       "      <td>0.610501</td>\n",
       "      <td>SVC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.153486</td>\n",
       "      <td>0.040867</td>\n",
       "      <td>0.488189</td>\n",
       "      <td>0.485887</td>\n",
       "      <td>0.488189</td>\n",
       "      <td>0.484584</td>\n",
       "      <td>0.523043</td>\n",
       "      <td>SVC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.150492</td>\n",
       "      <td>0.054817</td>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.544647</td>\n",
       "      <td>0.543307</td>\n",
       "      <td>0.539855</td>\n",
       "      <td>0.563271</td>\n",
       "      <td>SVC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.159470</td>\n",
       "      <td>0.055812</td>\n",
       "      <td>0.562992</td>\n",
       "      <td>0.569280</td>\n",
       "      <td>0.562992</td>\n",
       "      <td>0.558638</td>\n",
       "      <td>0.582701</td>\n",
       "      <td>SVC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.466535</td>\n",
       "      <td>0.491380</td>\n",
       "      <td>0.466535</td>\n",
       "      <td>0.444277</td>\n",
       "      <td>0.491542</td>\n",
       "      <td>GNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.004982</td>\n",
       "      <td>0.523622</td>\n",
       "      <td>0.521387</td>\n",
       "      <td>0.523622</td>\n",
       "      <td>0.518904</td>\n",
       "      <td>0.525320</td>\n",
       "      <td>GNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.509843</td>\n",
       "      <td>0.505928</td>\n",
       "      <td>0.509843</td>\n",
       "      <td>0.506454</td>\n",
       "      <td>0.524838</td>\n",
       "      <td>GNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.004984</td>\n",
       "      <td>0.562992</td>\n",
       "      <td>0.564130</td>\n",
       "      <td>0.562992</td>\n",
       "      <td>0.558016</td>\n",
       "      <td>0.573876</td>\n",
       "      <td>GNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.005981</td>\n",
       "      <td>0.525591</td>\n",
       "      <td>0.550480</td>\n",
       "      <td>0.525591</td>\n",
       "      <td>0.504332</td>\n",
       "      <td>0.546841</td>\n",
       "      <td>GNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.210297</td>\n",
       "      <td>0.097674</td>\n",
       "      <td>0.539370</td>\n",
       "      <td>0.539373</td>\n",
       "      <td>0.539370</td>\n",
       "      <td>0.539363</td>\n",
       "      <td>0.586521</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.394678</td>\n",
       "      <td>0.009967</td>\n",
       "      <td>0.568898</td>\n",
       "      <td>0.569048</td>\n",
       "      <td>0.568898</td>\n",
       "      <td>0.568953</td>\n",
       "      <td>0.575612</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.197340</td>\n",
       "      <td>0.006977</td>\n",
       "      <td>0.541339</td>\n",
       "      <td>0.541619</td>\n",
       "      <td>0.541339</td>\n",
       "      <td>0.541376</td>\n",
       "      <td>0.574685</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.187372</td>\n",
       "      <td>0.006977</td>\n",
       "      <td>0.541339</td>\n",
       "      <td>0.542733</td>\n",
       "      <td>0.541339</td>\n",
       "      <td>0.541429</td>\n",
       "      <td>0.559861</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.191360</td>\n",
       "      <td>0.007973</td>\n",
       "      <td>0.547244</td>\n",
       "      <td>0.547228</td>\n",
       "      <td>0.547244</td>\n",
       "      <td>0.547202</td>\n",
       "      <td>0.561435</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.599306</td>\n",
       "      <td>0.180397</td>\n",
       "      <td>0.519685</td>\n",
       "      <td>0.521893</td>\n",
       "      <td>0.519685</td>\n",
       "      <td>0.519983</td>\n",
       "      <td>0.538200</td>\n",
       "      <td>oRF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.669120</td>\n",
       "      <td>0.186378</td>\n",
       "      <td>0.564961</td>\n",
       "      <td>0.565087</td>\n",
       "      <td>0.564961</td>\n",
       "      <td>0.564969</td>\n",
       "      <td>0.596354</td>\n",
       "      <td>oRF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.266422</td>\n",
       "      <td>0.178389</td>\n",
       "      <td>0.580709</td>\n",
       "      <td>0.580622</td>\n",
       "      <td>0.580709</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.609186</td>\n",
       "      <td>oRF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.404912</td>\n",
       "      <td>0.202325</td>\n",
       "      <td>0.559055</td>\n",
       "      <td>0.559619</td>\n",
       "      <td>0.559055</td>\n",
       "      <td>0.558782</td>\n",
       "      <td>0.594370</td>\n",
       "      <td>oRF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2.567463</td>\n",
       "      <td>0.198327</td>\n",
       "      <td>0.541339</td>\n",
       "      <td>0.541426</td>\n",
       "      <td>0.541339</td>\n",
       "      <td>0.541330</td>\n",
       "      <td>0.558129</td>\n",
       "      <td>oRF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1416.796654</td>\n",
       "      <td>0.010961</td>\n",
       "      <td>0.594488</td>\n",
       "      <td>0.594541</td>\n",
       "      <td>0.594488</td>\n",
       "      <td>0.594432</td>\n",
       "      <td>0.615839</td>\n",
       "      <td>oXGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1377.439340</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>0.574803</td>\n",
       "      <td>0.576840</td>\n",
       "      <td>0.574803</td>\n",
       "      <td>0.575100</td>\n",
       "      <td>0.613930</td>\n",
       "      <td>oXGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1785.626089</td>\n",
       "      <td>0.014893</td>\n",
       "      <td>0.555118</td>\n",
       "      <td>0.555479</td>\n",
       "      <td>0.555118</td>\n",
       "      <td>0.554877</td>\n",
       "      <td>0.600973</td>\n",
       "      <td>oXGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1462.405595</td>\n",
       "      <td>0.009964</td>\n",
       "      <td>0.549213</td>\n",
       "      <td>0.556337</td>\n",
       "      <td>0.549213</td>\n",
       "      <td>0.547159</td>\n",
       "      <td>0.589272</td>\n",
       "      <td>oXGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1295.674848</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.594488</td>\n",
       "      <td>0.596434</td>\n",
       "      <td>0.594488</td>\n",
       "      <td>0.594715</td>\n",
       "      <td>0.619081</td>\n",
       "      <td>oXGB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fit_time  score_time  test_accuracy  test_precision_weighted  \\\n",
       "0      0.012955    0.005979       0.509843                 0.541768   \n",
       "1      0.003987    0.004985       0.498031                 0.248035   \n",
       "2      0.010964    0.003986       0.519685                 0.515188   \n",
       "3      0.002990    0.004982       0.517717                 0.268030   \n",
       "4      0.002991    0.004982       0.478346                 0.228815   \n",
       "5      0.586042    0.048870       0.505906                 0.506085   \n",
       "6      0.521223    0.040863       0.541339                 0.541697   \n",
       "7      0.540192    0.038906       0.566929                 0.567479   \n",
       "8      0.570056    0.052824       0.547244                 0.552169   \n",
       "9      0.747499    0.055812       0.559055                 0.560930   \n",
       "10     0.003986    0.022967       0.561024                 0.560879   \n",
       "11     0.002990    0.021918       0.561024                 0.560977   \n",
       "12     0.002997    0.021922       0.551181                 0.551414   \n",
       "13     0.002953    0.021970       0.568898                 0.569465   \n",
       "14     0.002985    0.021882       0.590551                 0.590551   \n",
       "15     0.149500    0.041860       0.537402                 0.549090   \n",
       "16     0.149498    0.043854       0.574803                 0.574395   \n",
       "17     0.153486    0.040867       0.488189                 0.485887   \n",
       "18     0.150492    0.054817       0.543307                 0.544647   \n",
       "19     0.159470    0.055812       0.562992                 0.569280   \n",
       "20     0.000996    0.005979       0.466535                 0.491380   \n",
       "21     0.000997    0.004982       0.523622                 0.521387   \n",
       "22     0.000996    0.004986       0.509843                 0.505928   \n",
       "23     0.000997    0.004984       0.562992                 0.564130   \n",
       "24     0.000994    0.005981       0.525591                 0.550480   \n",
       "25     0.210297    0.097674       0.539370                 0.539373   \n",
       "26     0.394678    0.009967       0.568898                 0.569048   \n",
       "27     0.197340    0.006977       0.541339                 0.541619   \n",
       "28     0.187372    0.006977       0.541339                 0.542733   \n",
       "29     0.191360    0.007973       0.547244                 0.547228   \n",
       "30     2.599306    0.180397       0.519685                 0.521893   \n",
       "31     2.669120    0.186378       0.564961                 0.565087   \n",
       "32     2.266422    0.178389       0.580709                 0.580622   \n",
       "33     2.404912    0.202325       0.559055                 0.559619   \n",
       "34     2.567463    0.198327       0.541339                 0.541426   \n",
       "35  1416.796654    0.010961       0.594488                 0.594541   \n",
       "36  1377.439340    0.012955       0.574803                 0.576840   \n",
       "37  1785.626089    0.014893       0.555118                 0.555479   \n",
       "38  1462.405595    0.009964       0.549213                 0.556337   \n",
       "39  1295.674848    0.010963       0.594488                 0.596434   \n",
       "\n",
       "    test_recall_weighted  test_f1_weighted  test_roc_auc   model  \n",
       "0               0.509843          0.492145      0.528148  LogReg  \n",
       "1               0.498031          0.331148      0.553298  LogReg  \n",
       "2               0.519685          0.510596      0.516109  LogReg  \n",
       "3               0.517717          0.353202      0.565981  LogReg  \n",
       "4               0.478346          0.309556      0.578803  LogReg  \n",
       "5               0.505906          0.505758      0.534714      RF  \n",
       "6               0.541339          0.541045      0.576335      RF  \n",
       "7               0.566929          0.567063      0.586172      RF  \n",
       "8               0.547244          0.547244      0.562988      RF  \n",
       "9               0.559055          0.559438      0.571122      RF  \n",
       "10              0.561024          0.560845      0.565667     KNN  \n",
       "11              0.561024          0.560214      0.570876     KNN  \n",
       "12              0.551181          0.550261      0.576843     KNN  \n",
       "13              0.568898          0.569073      0.561635     KNN  \n",
       "14              0.590551          0.590551      0.599347     KNN  \n",
       "15              0.537402          0.534247      0.539116     SVC  \n",
       "16              0.574803          0.571686      0.610501     SVC  \n",
       "17              0.488189          0.484584      0.523043     SVC  \n",
       "18              0.543307          0.539855      0.563271     SVC  \n",
       "19              0.562992          0.558638      0.582701     SVC  \n",
       "20              0.466535          0.444277      0.491542     GNB  \n",
       "21              0.523622          0.518904      0.525320     GNB  \n",
       "22              0.509843          0.506454      0.524838     GNB  \n",
       "23              0.562992          0.558016      0.573876     GNB  \n",
       "24              0.525591          0.504332      0.546841     GNB  \n",
       "25              0.539370          0.539363      0.586521     XGB  \n",
       "26              0.568898          0.568953      0.575612     XGB  \n",
       "27              0.541339          0.541376      0.574685     XGB  \n",
       "28              0.541339          0.541429      0.559861     XGB  \n",
       "29              0.547244          0.547202      0.561435     XGB  \n",
       "30              0.519685          0.519983      0.538200     oRF  \n",
       "31              0.564961          0.564969      0.596354     oRF  \n",
       "32              0.580709          0.580645      0.609186     oRF  \n",
       "33              0.559055          0.558782      0.594370     oRF  \n",
       "34              0.541339          0.541330      0.558129     oRF  \n",
       "35              0.594488          0.594432      0.615839    oXGB  \n",
       "36              0.574803          0.575100      0.613930    oXGB  \n",
       "37              0.555118          0.554877      0.600973    oXGB  \n",
       "38              0.549213          0.547159      0.589272    oXGB  \n",
       "39              0.594488          0.594715      0.619081    oXGB  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = compare_models(X_train, np.ravel(y_train, order='C'), X_test, y_test)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "9Zhw9vgmkBlM"
   },
   "outputs": [],
   "source": [
    "# dynamically generate columns for visualizing\n",
    "\n",
    "bootstraps = []\n",
    "for model in list(set(final.model.values)):\n",
    "    model_df = final.loc[final.model == model]\n",
    "    bootstrap = model_df.sample(n=30, replace=True)\n",
    "    bootstraps.append(bootstrap)\n",
    "        \n",
    "bootstrap_df = pd.concat(bootstraps, ignore_index=True)\n",
    "results_long = pd.melt(bootstrap_df,id_vars=['model'],var_name='metrics', value_name='values')\n",
    "time_metrics = ['fit_time','score_time'] # fit time metrics\n",
    "\n",
    "## PERFORMANCE METRICS\n",
    "results_long_nofit = results_long.loc[~results_long['metrics'].isin(time_metrics)] # get df without fit data\n",
    "results_long_nofit = results_long_nofit.sort_values(by='values')\n",
    "\n",
    "## TIME METRICS\n",
    "results_long_fit = results_long.loc[results_long['metrics'].isin(time_metrics)] # df with fit data\n",
    "results_long_fit = results_long_fit.sort_values(by='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 800
    },
    "id": "Cj7iJB39kywZ",
    "outputId": "29adef27-480c-416f-f5c5-768ae4160a97",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Model Comparisons')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing model classifier performance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(20, 12))\n",
    "fig = sns.boxplot(x=\"model\", y=\"values\", hue=\"metrics\", data=results_long_nofit, palette=\"Set3\")\n",
    "plt.legend(loc=0)\n",
    "plt.title('Model Comparisons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 800
    },
    "id": "Qtr7sfSwk7sE",
    "outputId": "c36ecf02-81f0-401b-fe67-0f2601c35ce2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Model Training & Prediction Time Comparison')"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing training and scoring times\n",
    "plt.figure(figsize=(20, 12))\n",
    "g = sns.boxplot(x=\"model\", y=\"values\", hue=\"metrics\", data=results_long_fit, palette=\"Set3\")\n",
    "plt.legend(loc=0)\n",
    "plt.title('Model Training & Prediction Time Comparison')\n",
    "\n",
    "# notes for further analysis -- consider Neural Network which generally takes large amounts of data/time to train, but might yield better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "53QpkTD-nWgm",
    "outputId": "43f1aa52-c5e1-45f2-e3da-8ee772b7a8ce"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">test_precision_weighted</th>\n",
       "      <th colspan=\"2\" halign=\"left\">test_roc_auc</th>\n",
       "      <th colspan=\"2\" halign=\"left\">test_accuracy</th>\n",
       "      <th colspan=\"2\" halign=\"left\">test_f1_weighted</th>\n",
       "      <th colspan=\"2\" halign=\"left\">test_recall_weighted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GNB</th>\n",
       "      <td>0.026927</td>\n",
       "      <td>0.532571</td>\n",
       "      <td>0.027557</td>\n",
       "      <td>0.536711</td>\n",
       "      <td>0.030908</td>\n",
       "      <td>0.522441</td>\n",
       "      <td>0.036474</td>\n",
       "      <td>0.510462</td>\n",
       "      <td>0.030908</td>\n",
       "      <td>0.522441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.568557</td>\n",
       "      <td>0.014594</td>\n",
       "      <td>0.574770</td>\n",
       "      <td>0.013487</td>\n",
       "      <td>0.568438</td>\n",
       "      <td>0.013743</td>\n",
       "      <td>0.568201</td>\n",
       "      <td>0.013487</td>\n",
       "      <td>0.568438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg</th>\n",
       "      <td>0.138893</td>\n",
       "      <td>0.351514</td>\n",
       "      <td>0.024331</td>\n",
       "      <td>0.552253</td>\n",
       "      <td>0.016930</td>\n",
       "      <td>0.504003</td>\n",
       "      <td>0.084684</td>\n",
       "      <td>0.393379</td>\n",
       "      <td>0.016930</td>\n",
       "      <td>0.504003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.023254</td>\n",
       "      <td>0.547290</td>\n",
       "      <td>0.019237</td>\n",
       "      <td>0.570033</td>\n",
       "      <td>0.022966</td>\n",
       "      <td>0.546391</td>\n",
       "      <td>0.023111</td>\n",
       "      <td>0.546393</td>\n",
       "      <td>0.022966</td>\n",
       "      <td>0.546391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.032079</td>\n",
       "      <td>0.545288</td>\n",
       "      <td>0.029967</td>\n",
       "      <td>0.561716</td>\n",
       "      <td>0.029809</td>\n",
       "      <td>0.541010</td>\n",
       "      <td>0.029707</td>\n",
       "      <td>0.537371</td>\n",
       "      <td>0.029809</td>\n",
       "      <td>0.541010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>0.011318</td>\n",
       "      <td>0.547103</td>\n",
       "      <td>0.008741</td>\n",
       "      <td>0.574772</td>\n",
       "      <td>0.011393</td>\n",
       "      <td>0.546785</td>\n",
       "      <td>0.011403</td>\n",
       "      <td>0.546817</td>\n",
       "      <td>0.011393</td>\n",
       "      <td>0.546785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oRF</th>\n",
       "      <td>0.019835</td>\n",
       "      <td>0.553453</td>\n",
       "      <td>0.026477</td>\n",
       "      <td>0.579896</td>\n",
       "      <td>0.020530</td>\n",
       "      <td>0.552822</td>\n",
       "      <td>0.020392</td>\n",
       "      <td>0.552788</td>\n",
       "      <td>0.020530</td>\n",
       "      <td>0.552822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oXGB</th>\n",
       "      <td>0.017111</td>\n",
       "      <td>0.577259</td>\n",
       "      <td>0.010664</td>\n",
       "      <td>0.608965</td>\n",
       "      <td>0.018438</td>\n",
       "      <td>0.575131</td>\n",
       "      <td>0.019016</td>\n",
       "      <td>0.574852</td>\n",
       "      <td>0.018438</td>\n",
       "      <td>0.575131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_precision_weighted           test_roc_auc           test_accuracy  \\\n",
       "                           std      mean          std      mean           std   \n",
       "model                                                                           \n",
       "GNB                   0.026927  0.532571     0.027557  0.536711      0.030908   \n",
       "KNN                   0.013479  0.568557     0.014594  0.574770      0.013487   \n",
       "LogReg                0.138893  0.351514     0.024331  0.552253      0.016930   \n",
       "RF                    0.023254  0.547290     0.019237  0.570033      0.022966   \n",
       "SVC                   0.032079  0.545288     0.029967  0.561716      0.029809   \n",
       "XGB                   0.011318  0.547103     0.008741  0.574772      0.011393   \n",
       "oRF                   0.019835  0.553453     0.026477  0.579896      0.020530   \n",
       "oXGB                  0.017111  0.577259     0.010664  0.608965      0.018438   \n",
       "\n",
       "                 test_f1_weighted           test_recall_weighted            \n",
       "            mean              std      mean                  std      mean  \n",
       "model                                                                       \n",
       "GNB     0.522441         0.036474  0.510462             0.030908  0.522441  \n",
       "KNN     0.568438         0.013743  0.568201             0.013487  0.568438  \n",
       "LogReg  0.504003         0.084684  0.393379             0.016930  0.504003  \n",
       "RF      0.546391         0.023111  0.546393             0.022966  0.546391  \n",
       "SVC     0.541010         0.029707  0.537371             0.029809  0.541010  \n",
       "XGB     0.546785         0.011403  0.546817             0.011393  0.546785  \n",
       "oRF     0.552822         0.020392  0.552788             0.020530  0.552822  \n",
       "oXGB    0.575131         0.019016  0.574852             0.018438  0.575131  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = list(set(results_long_nofit.metrics.values))\n",
    "bootstrap_df.groupby(['model'])[metrics].agg([np.std, np.mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "ad8_-TeuPLcp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our oXGB model correctly predicted 49.997033345199945% results\n"
     ]
    }
   ],
   "source": [
    "oxgb_y_pred = best_xgb.predict(X_test)\n",
    "error = np.mean(oxgb_y_pred != y_test)\n",
    "print(f'Our oXGB model correctly predicted {round((1-error)*100,2)}% results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "nlbxnZQsPL37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our oXGB model correctly predicted 49.99% results\n"
     ]
    }
   ],
   "source": [
    "cls=RandomForestClassifier()\n",
    "cls.fit(X_train, np.ravel(y_train,order='C'))\n",
    "y_pred = cls.predict(X_test)\n",
    "\n",
    "error = np.mean(y_pred != y_test)\n",
    "print(f'Our oXGB model correctly predicted {round((1-error)*100,2)}% results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our oXGB model correctly predicted 50.0% results\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = fitted_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### aditional modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date_price_merged2 = pd.concat([date_price_merged3, sentiment_scores_df2], axis=1, join='inner')\n",
    "date_price_merged2.drop(columns=['normalised_date','daily_log_diff','rolling','dt_index'], inplace=True)\n",
    "\n",
    "# creating training and testing datasets\n",
    "df = date_price_merged2\n",
    "\n",
    "X = df.loc[:,df.columns != 'target'].select_dtypes(include=[np.number]).values\n",
    "y = df.loc[:,['target']].values\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3176, 7)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "## multilayer perceptron\n",
    "import tensorflow as tf \n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(100, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(50, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.relu))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "80/80 [==============================] - 0s 622us/step - loss: 0.2599\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 610us/step - loss: 0.2465\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.2449\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.2453\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.2437\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 560us/step - loss: 0.2436\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 563us/step - loss: 0.2425\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.2421\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.2451\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.2444\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 598us/step - loss: 0.2423\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.2431\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.2426\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 611us/step - loss: 0.2416\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 612us/step - loss: 0.2411\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 598us/step - loss: 0.2402\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.2410\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 598us/step - loss: 0.2419\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 580us/step - loss: 0.2398\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 587us/step - loss: 0.2410\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.2401\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.2399\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.2393\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 623us/step - loss: 0.2393\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 706us/step - loss: 0.2403\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 623us/step - loss: 0.2385\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.2402\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.2385\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 680us/step - loss: 0.2389\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 685us/step - loss: 0.2379\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 695us/step - loss: 0.2393\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 658us/step - loss: 0.2386\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 623us/step - loss: 0.2402\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.2398\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 537us/step - loss: 0.2371\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.2389\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.2374\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.2378\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 698us/step - loss: 0.2378\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 685us/step - loss: 0.2372\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 623us/step - loss: 0.2401\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.2353\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.2374\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 623us/step - loss: 0.2366\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 540us/step - loss: 0.2367\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 539us/step - loss: 0.2368\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.2366\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 624us/step - loss: 0.2374\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 620us/step - loss: 0.2379\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.2361\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 639us/step - loss: 0.2378\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s 760us/step - loss: 0.2357\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s 648us/step - loss: 0.2357\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 0s 773us/step - loss: 0.2350\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 0s 635us/step - loss: 0.2346\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 0s 739us/step - loss: 0.2350\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 0s 648us/step - loss: 0.2348\n",
      "Epoch 58/100\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.2343\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 0s 505us/step - loss: 0.2352\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.2346\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.2357\n",
      "Epoch 62/100\n",
      "80/80 [==============================] - 0s 519us/step - loss: 0.2345\n",
      "Epoch 63/100\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.2343\n",
      "Epoch 64/100\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.2335\n",
      "Epoch 65/100\n",
      "80/80 [==============================] - 0s 603us/step - loss: 0.2333\n",
      "Epoch 66/100\n",
      "80/80 [==============================] - 0s 623us/step - loss: 0.2337\n",
      "Epoch 67/100\n",
      "80/80 [==============================] - 0s 643us/step - loss: 0.2339\n",
      "Epoch 68/100\n",
      "80/80 [==============================] - 0s 610us/step - loss: 0.2350\n",
      "Epoch 69/100\n",
      "80/80 [==============================] - 0s 570us/step - loss: 0.2340\n",
      "Epoch 70/100\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.2326\n",
      "Epoch 71/100\n",
      "80/80 [==============================] - 0s 538us/step - loss: 0.2340\n",
      "Epoch 72/100\n",
      "80/80 [==============================] - 0s 511us/step - loss: 0.2336\n",
      "Epoch 73/100\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.2350\n",
      "Epoch 74/100\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.2341\n",
      "Epoch 75/100\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.2329\n",
      "Epoch 76/100\n",
      "80/80 [==============================] - 0s 643us/step - loss: 0.2337\n",
      "Epoch 77/100\n",
      "80/80 [==============================] - 0s 701us/step - loss: 0.2336\n",
      "Epoch 78/100\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.2356\n",
      "Epoch 79/100\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.2355\n",
      "Epoch 80/100\n",
      "80/80 [==============================] - 0s 515us/step - loss: 0.2330\n",
      "Epoch 81/100\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.2326\n",
      "Epoch 82/100\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.2324\n",
      "Epoch 83/100\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.2325\n",
      "Epoch 84/100\n",
      "80/80 [==============================] - 0s 696us/step - loss: 0.2345\n",
      "Epoch 85/100\n",
      "80/80 [==============================] - 0s 648us/step - loss: 0.2333\n",
      "Epoch 86/100\n",
      "80/80 [==============================] - 0s 610us/step - loss: 0.2334\n",
      "Epoch 87/100\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.2310\n",
      "Epoch 88/100\n",
      "80/80 [==============================] - 0s 498us/step - loss: 0.2307\n",
      "Epoch 89/100\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.2330\n",
      "Epoch 90/100\n",
      "80/80 [==============================] - 0s 524us/step - loss: 0.2315\n",
      "Epoch 91/100\n",
      "80/80 [==============================] - 0s 533us/step - loss: 0.2301\n",
      "Epoch 92/100\n",
      "80/80 [==============================] - 0s 499us/step - loss: 0.2318\n",
      "Epoch 93/100\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.2307\n",
      "Epoch 94/100\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.2308\n",
      "Epoch 95/100\n",
      "80/80 [==============================] - 0s 548us/step - loss: 0.2326\n",
      "Epoch 96/100\n",
      "80/80 [==============================] - 0s 588us/step - loss: 0.2319\n",
      "Epoch 97/100\n",
      "80/80 [==============================] - 0s 536us/step - loss: 0.2331\n",
      "Epoch 98/100\n",
      "80/80 [==============================] - 0s 525us/step - loss: 0.2317\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 536us/step - loss: 0.2306\n",
      "Epoch 100/100\n",
      "80/80 [==============================] - 0s 550us/step - loss: 0.2324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f083a54a48>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 847us/step - loss: 0.2379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23790627717971802"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 2/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 3/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 4/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 5/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 6/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 7/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 8/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 9/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 10/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 11/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 12/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 13/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 14/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 15/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 16/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 17/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 18/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 19/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 20/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 21/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 22/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 23/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 24/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 25/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 26/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 27/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 28/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 29/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 30/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 31/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 32/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 33/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 34/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 35/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 36/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 37/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 38/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 39/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 40/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 41/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 42/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 43/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 44/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 45/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 46/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 47/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 48/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 49/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "Epoch 50/50\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.5087\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4858490526676178"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_shape = np.reshape(X_train, X_train.shape + (1,))\n",
    "y_shape = np.reshape(y_train, y_train.shape + (1,))\n",
    "x_shaped = np.reshape(X_test, X_test.shape + (1,))\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(50, input_shape=(7,1), return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(20))\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.relu))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "model.fit(x_shape, y_train, epochs=50)\n",
    "\n",
    "model.evaluate(x_shaped, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_47 (LSTM)               (None, 7, 50)             10400     \n",
      "_________________________________________________________________\n",
      "lstm_48 (LSTM)               (None, 20)                5680      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 16,101\n",
      "Trainable params: 16,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our ANN correctly predicted 51.42% results\n"
     ]
    }
   ],
   "source": [
    "error = np.mean(y_pred != y_test)\n",
    "print(f'Our ANN correctly predicted {round((1-error)*100,2)}% results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 2/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 3/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 4/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 5/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 6/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 7/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 8/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 9/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 10/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 11/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 12/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 13/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 14/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 15/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 16/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 17/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 18/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 19/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 20/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 21/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 22/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 23/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 24/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 25/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 26/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764A: 0s - loss: 0.4375 - acc: 0.562\n",
      "Epoch 27/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 28/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 29/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 30/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 31/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 32/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 33/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 34/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 35/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 36/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 37/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 38/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 39/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 40/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 41/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 42/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 43/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 44/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 45/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 46/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 47/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 48/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 49/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 50/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 51/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 52/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 53/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 54/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 55/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 56/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 57/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 58/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 59/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 60/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 61/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 62/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 63/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 64/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 65/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 66/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 67/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 68/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 69/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 70/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 71/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 72/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 73/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 74/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 75/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 76/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 77/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 78/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 79/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 80/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 81/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 82/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 83/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 84/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 85/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 86/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 88/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 89/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 90/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 91/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 92/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 93/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 94/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 95/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 96/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 97/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 98/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 99/850\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 100/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 101/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 102/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 103/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 104/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 105/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 106/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 107/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 108/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 109/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 110/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 111/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 112/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 113/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 114/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 115/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 116/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 117/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 118/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 119/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 120/850\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 121/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 122/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 123/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 124/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 125/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 126/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 127/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 128/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 129/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 130/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 131/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 132/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 133/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 134/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 135/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 136/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 137/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 138/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 139/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 140/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 141/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 142/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 143/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 144/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 145/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 146/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 147/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 148/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 149/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 150/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 151/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 152/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 153/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 154/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 155/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 156/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 157/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 158/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 159/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 160/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 161/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 162/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 163/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 164/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 165/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 166/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 167/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 168/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 169/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 170/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 171/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 172/850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 173/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 174/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 175/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 176/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 177/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 178/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 179/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 180/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 181/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 182/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 183/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 184/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 185/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 186/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 187/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 188/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 189/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 190/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 191/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 192/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 193/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 194/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 195/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 196/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 197/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 198/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 199/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 200/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 201/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 202/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 203/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 204/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 205/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 206/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 207/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 208/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 209/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 210/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 211/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 212/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 213/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 214/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 215/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 216/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 217/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 218/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 219/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 220/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 221/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 222/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 223/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 224/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 225/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 226/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 227/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 228/850\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 229/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 230/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 231/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 232/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 233/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 234/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 235/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 236/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 237/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 238/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 239/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 240/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 241/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 242/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 243/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 244/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 245/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 246/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 247/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 248/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 249/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 250/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 251/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 252/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 253/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 254/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 255/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 256/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 257/850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 258/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 259/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 260/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 261/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 262/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 263/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 264/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 265/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 266/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 267/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 268/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 269/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 270/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 271/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 272/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 273/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 274/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 275/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 276/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 277/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 278/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 279/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 280/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 281/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 282/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 283/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 284/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 285/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 286/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 287/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 288/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 289/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 290/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 291/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 292/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 293/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 294/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 295/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 296/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 297/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 298/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 299/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 300/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 301/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 302/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 303/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 304/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 305/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 306/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 307/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 308/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 309/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 310/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 311/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 312/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 313/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 314/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 315/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 316/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 317/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 318/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 319/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 320/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 321/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 322/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 323/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 324/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 325/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 326/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 327/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 328/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 329/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 330/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 331/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 332/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 333/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 334/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 335/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 336/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 337/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 338/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 339/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 340/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 341/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 342/850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 343/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 344/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 345/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 346/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 347/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 348/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 349/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 350/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 351/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 352/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 353/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 354/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 355/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 356/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 357/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 358/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 359/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 360/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 361/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 362/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 363/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 364/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 365/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 366/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 367/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 368/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 369/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 370/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 371/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 372/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 373/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 374/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 375/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 376/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 377/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 378/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 379/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 380/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 381/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 382/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 383/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 384/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 385/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 386/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 387/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 388/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 389/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 390/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 391/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 392/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 393/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 394/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 395/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 396/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 397/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 398/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 399/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 400/850\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.4196 - acc: 0.580 - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 401/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 402/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 403/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 404/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 405/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 406/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 407/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 408/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 409/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 410/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 411/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 412/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 413/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 414/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 415/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 416/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 417/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 418/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 419/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 420/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 421/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 422/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 423/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 424/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 425/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 426/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 427/850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 428/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 429/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 430/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 431/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 432/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 433/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 434/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 435/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 436/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 437/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 438/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 439/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 440/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 441/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 442/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 443/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 444/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 445/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 446/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 447/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 448/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 449/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 450/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 451/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 452/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 453/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 454/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 455/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 456/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 457/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 458/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 459/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 460/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 461/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 462/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 463/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 464/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 465/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 466/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 467/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 468/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 469/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 470/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 471/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 472/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 473/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 474/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 475/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 476/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 477/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 478/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 479/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 480/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 481/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 482/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 483/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 484/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 485/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 486/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 487/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 488/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 489/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 490/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 491/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 492/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 493/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 494/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 495/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 496/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 497/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 498/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 499/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 500/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 501/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 502/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 503/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 504/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 505/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 506/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 507/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 508/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 509/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 510/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 511/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 512/850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 513/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 514/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 515/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 516/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 517/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 518/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 519/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 520/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 521/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 522/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 523/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 524/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 525/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 526/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 527/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 528/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 529/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 530/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 531/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 532/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 533/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 534/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 535/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 536/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 537/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 538/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 539/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 540/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 541/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 542/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 543/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 544/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 545/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 546/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 547/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 548/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 549/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 550/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 551/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 552/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 553/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 554/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 555/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 556/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 557/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 558/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 559/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 560/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 561/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 562/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 563/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 564/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 565/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 566/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 567/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 568/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 569/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 570/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 571/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 572/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 573/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 574/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 575/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 576/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 577/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 578/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 579/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 580/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 581/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 582/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 583/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 584/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 585/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 586/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 587/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 588/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 589/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 590/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 591/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 592/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 593/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 594/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 595/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 596/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 597/850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 598/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 599/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 600/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 601/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 602/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 603/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 604/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 605/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 606/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 607/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 608/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 609/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 610/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 611/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 612/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 613/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 614/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 615/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 616/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 617/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 618/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 619/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 620/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 621/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 622/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 623/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 624/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 625/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 626/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 627/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 628/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 629/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 630/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 631/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 632/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 633/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 634/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 635/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 636/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 637/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 638/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 639/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 640/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 641/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 642/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 643/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 644/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 645/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 646/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 647/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 648/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 649/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 650/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 651/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 652/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 653/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 654/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 655/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 656/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 657/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 658/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 659/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 660/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 661/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 662/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 663/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 664/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 665/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 666/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 667/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 668/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 669/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 670/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 671/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 672/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 673/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 674/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 675/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 676/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 677/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 678/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 679/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 680/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 681/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 682/850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 683/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 684/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 685/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 686/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 687/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 688/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 689/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 690/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 691/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 692/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 693/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 694/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 695/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 696/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 697/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 698/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 699/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 700/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 701/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 702/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 703/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 704/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 705/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 706/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 707/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 708/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 709/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 710/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 711/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 712/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 713/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 714/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 715/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 716/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 717/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 718/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 719/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 720/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 721/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 722/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 723/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 724/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 725/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 726/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 727/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 728/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 729/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 730/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 731/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 732/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 733/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 734/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 735/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 736/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 737/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 738/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 739/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 740/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 741/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 742/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 743/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 744/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 745/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 746/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 747/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 748/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 749/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 750/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 751/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 752/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 753/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 754/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 755/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 756/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 757/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 758/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 759/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 760/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 761/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 762/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 763/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 764/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 765/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 766/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 767/850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 768/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 769/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 770/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 771/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 772/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 773/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 774/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 775/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 776/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 777/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 778/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 779/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 780/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 781/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 782/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 783/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 784/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 785/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 786/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 787/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 788/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 789/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 790/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 791/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 792/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 793/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 794/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 795/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 796/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 797/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 798/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 799/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 800/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 801/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 802/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 803/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 804/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 805/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 806/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 807/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 808/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 809/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 810/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 811/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 812/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 813/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 814/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 815/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 816/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 817/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 818/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 819/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 820/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 821/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 822/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 823/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 824/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 825/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 826/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 827/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 828/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 829/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 830/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 831/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 832/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 833/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 834/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 835/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 836/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 837/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 838/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 839/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 840/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 841/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 842/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 843/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 844/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 845/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 846/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 847/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 848/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 849/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "Epoch 850/850\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.4236 - acc: 0.5764\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5242 - acc: 0.4758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5242030620574951, 0.4757969379425049]"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_shape = np.reshape(X_train, X_train.shape + (1,))\n",
    "#y_shape = np.reshape(y_train, y_train.shape + (1,))\n",
    "#x_shaped = np.reshape(X_test, X_test.shape + (1,))\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(128, input_shape=(6,1), return_sequences=False))\n",
    "model.add(tf.keras.layers.Dense(105, activation=tf.nn.sigmoid))\n",
    "model.add(tf.keras.layers.Dense(80, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(35, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.relu))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['acc'])\n",
    "\n",
    "model.fit(x_shape, y_shape, epochs=850)\n",
    "\n",
    "model.evaluate(x_shaped, y_shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our RNN correctly predicted 47.58% results\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_shaped)\n",
    "threshold = 0.5\n",
    "predictions = [1 if val > threshold else 0 for val in y_pred]\n",
    "error = np.mean(predictions != y_test)\n",
    "print(f'Our RNN correctly predicted {round((1-error)*100,2)}% results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weightedPrice</th>\n",
       "      <th>daily_pct_change</th>\n",
       "      <th>target</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>rolling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-02</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.040222</td>\n",
       "      <td>0</td>\n",
       "      <td>0.079707</td>\n",
       "      <td>0.101561</td>\n",
       "      <td>0.078388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-03</th>\n",
       "      <td>5.252500</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.102477</td>\n",
       "      <td>0.035952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04</th>\n",
       "      <td>5.208159</td>\n",
       "      <td>-0.008442</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078175</td>\n",
       "      <td>0.088750</td>\n",
       "      <td>0.051089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-05</th>\n",
       "      <td>6.284127</td>\n",
       "      <td>0.206593</td>\n",
       "      <td>1</td>\n",
       "      <td>0.058146</td>\n",
       "      <td>0.094976</td>\n",
       "      <td>0.195525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-06</th>\n",
       "      <td>6.438999</td>\n",
       "      <td>0.024645</td>\n",
       "      <td>1</td>\n",
       "      <td>0.063638</td>\n",
       "      <td>0.110809</td>\n",
       "      <td>0.185822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-07</th>\n",
       "      <td>10339.359896</td>\n",
       "      <td>0.014840</td>\n",
       "      <td>1</td>\n",
       "      <td>0.068073</td>\n",
       "      <td>0.106390</td>\n",
       "      <td>0.155254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-08</th>\n",
       "      <td>10303.242947</td>\n",
       "      <td>-0.003493</td>\n",
       "      <td>1</td>\n",
       "      <td>0.067413</td>\n",
       "      <td>0.108853</td>\n",
       "      <td>0.182260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-09</th>\n",
       "      <td>10373.259995</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>0</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.103004</td>\n",
       "      <td>0.160828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-10</th>\n",
       "      <td>10393.662602</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0</td>\n",
       "      <td>0.065841</td>\n",
       "      <td>0.098622</td>\n",
       "      <td>0.165506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-11</th>\n",
       "      <td>10332.429402</td>\n",
       "      <td>-0.005891</td>\n",
       "      <td>0</td>\n",
       "      <td>0.068309</td>\n",
       "      <td>0.109898</td>\n",
       "      <td>0.141176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3176 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            weightedPrice  daily_pct_change  target       neg       pos  \\\n",
       "2012-01-02       5.000000          0.040222       0  0.079707  0.101561   \n",
       "2012-01-03       5.252500          0.050500       0  0.080000  0.102477   \n",
       "2012-01-04       5.208159         -0.008442       1  0.078175  0.088750   \n",
       "2012-01-05       6.284127          0.206593       1  0.058146  0.094976   \n",
       "2012-01-06       6.438999          0.024645       1  0.063638  0.110809   \n",
       "...                   ...               ...     ...       ...       ...   \n",
       "2020-09-07   10339.359896          0.014840       1  0.068073  0.106390   \n",
       "2020-09-08   10303.242947         -0.003493       1  0.067413  0.108853   \n",
       "2020-09-09   10373.259995          0.006796       0  0.065800  0.103004   \n",
       "2020-09-10   10393.662602          0.001967       0  0.065841  0.098622   \n",
       "2020-09-11   10332.429402         -0.005891       0  0.068309  0.109898   \n",
       "\n",
       "             rolling  \n",
       "2012-01-02  0.078388  \n",
       "2012-01-03  0.035952  \n",
       "2012-01-04  0.051089  \n",
       "2012-01-05  0.195525  \n",
       "2012-01-06  0.185822  \n",
       "...              ...  \n",
       "2020-09-07  0.155254  \n",
       "2020-09-08  0.182260  \n",
       "2020-09-09  0.160828  \n",
       "2020-09-10  0.165506  \n",
       "2020-09-11  0.141176  \n",
       "\n",
       "[3176 rows x 6 columns]"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_price_merged2 = pd.concat([date_price_merged3, sentiment_scores_df2], axis=1, join='inner')\n",
    "date_price_merged2.drop(columns=['normalised_date','daily_log_diff','score','neu','compound','dt_index'], inplace=True)\n",
    "date_price_merged2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(635, 1)"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = date_price_merged2\n",
    "\n",
    "X = df.loc[:,df.columns != 'target'].select_dtypes(include=[np.number]).values\n",
    "y = df.loc[:,['target']].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(X)\n",
    "\n",
    "div = int(round(len(X) * 0.2))\n",
    "X_train = scaled[div:]\n",
    "y_train = y[div:]\n",
    "X_test = scaled[:div]\n",
    "y_test= y[:div]\n",
    "\n",
    "\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_shape = tf.keras.utils.to_categorical(y_train, 5)\n",
    "#y_shaped = tf.keras.utils.to_categorical(y_test, 5)\n",
    "\n",
    "X_train = np.reshape(X_train, X_train.shape + (1,))\n",
    "#y_train = np.reshape(y_shape, y_shape.shape + (1,))\n",
    "X_test = np.reshape(X_test, X_test.shape + (1,))\n",
    "#y_test = np.reshape(y_shaped, y_shaped.shape + (1,))\n",
    "\n",
    "#y_shape = tf.keras.utils.to_categorical(y_shape, 7)\n",
    "#y_shaped = tf.keras.utils.to_categorical(y_shaped, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_43 (Bidirectio (None, 5, 20)             992       \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 5, 12)             252       \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 5, 1)              13        \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 5, 1)              0         \n",
      "=================================================================\n",
      "Total params: 1,257\n",
      "Trainable params: 1,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# With custom backward layer\n",
    "model = tf.keras.Sequential()\n",
    "forward_layer = tf.keras.layers.LSTM(12, return_sequences=True)\n",
    "backward_layer = tf.keras.layers.LSTM(8, activation='relu', return_sequences=True,\n",
    "                   go_backwards=True)\n",
    "model.add(tf.keras.layers.Bidirectional(layer=forward_layer, backward_layer=backward_layer, input_shape=(5,1)))\n",
    "model.add(tf.keras.layers.Dense(12))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2541, 5, 1)"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1535 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4687 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 1) and (None, 5) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-542-a40627ed38ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# validation_data=(X_test,y_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m365\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 697\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py:1535 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4687 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    C:\\Users\\Patri\\anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 1) and (None, 5) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# validation_data=(X_test,y_test)\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=365)\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(635, 5, 1)"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "635"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.bool_' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-526-00756ff175d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Our RNN correctly predicted {round((1-error)*100,2)}% results'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.bool_' object is not iterable"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "threshold = 0.5\n",
    "predictions = [1 if val > threshold else 0 for val in y_pred.all()]\n",
    "error = np.mean(predictions != y_test)\n",
    "print(f'Our RNN correctly predicted {round((1-error)*100,2)}% results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BTC_increase_decrease_predictor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
